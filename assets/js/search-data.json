{
  
    
        "post0": {
            "title": "AMMI notes - MLE and MAP",
            "content": "Linear regression works by finding $ underset{ theta}{argmin} |{X theta - Y} |_2^2$. The interpretations for the metric we are minimizing are diverse. It can be interpreted as the sum of the euclidian distances between the predictions and actual values. It can also be obtained by maximum likelihood estimation (MLE). . Maximum Likelihood Estimation . In MLE we are interested in finding the value of theta that maximizes the likelihood of the data as expressed by $p(Y|X theta)$ . Let&#39;s consider every data point to be of the form $y_i = x_i theta + epsilon$ where $ epsilon$ follows a normal distribution $ mathcal{N}(0, , sigma^{2})$ and $y_i$ will follow a distribution $ mathcal{N}(x_i theta, , sigma^{2})$ . Our goal is now to find $ underset{ theta}{argmax} p(Y|X, theta)$ . Because we assume that the data points are independently sampled we can write $p(Y|X, theta) = displaystyle prod_{i=1}^{n} P(y_i|x_i, theta) $ . Then because we assume the data points are identically distributed, in this case with a distribution $ mathcal{N}(x_i theta, , sigma^{2})$, we can write $ displaystyle prod_{i=1}^{n} P(y_i|x_i, theta) = displaystyle prod_{i=1}^{n} frac{1}{ sqrt{2 pi sigma^2}}exp(- frac{(y_i - x_i theta)^2}{2 sigma^2} $ . Lastly, because the $ log$ function is a strictly increasing function, maximizing the likelihood $p(Y|X, theta)$ is the same as maximizing the log likelihood $ log p(Y|X, theta)$. . Using that let&#39;s develop the expression of the likelihood. $$ begin{aligned} p(Y|X, theta) &amp; = displaystyle prod_{i=1}^{n} P(y_i|x_i, theta) &amp; = displaystyle prod_{i=1}^{n} frac{1}{ sqrt{2 pi sigma^2}}exp(- frac{(y_i - x_i theta)^2}{2 sigma^2} ) log p(Y|X, theta) &amp; = displaystyle sum_{i=1}^{n} log ( frac{1}{ sqrt{2 pi sigma^2}}exp(- frac{(y_i - x_i theta)^2}{2 sigma^2} )) &amp; = displaystyle sum_{i=1}^{n} log frac{1}{ sqrt{2 pi sigma^2}} - displaystyle sum_{i=1}^{n} frac{(y_i - x_i theta)^2}{2 sigma^2} &amp; = n log 1 - n log sqrt{2 pi sigma^2} - frac{1}{2 sigma^2} displaystyle sum_{i=1}^{n} (y_i - x_i theta)^2 &amp; = n log 1 - n log sqrt{2 pi sigma^2} - frac{1}{2 sigma^2} |{Y - X theta } |_2^2 end{aligned} $$ . From that we can see that fincing the value of $ theta$ that maximizes the likelihood is the same as finding the $ theta$ that minimizes $ |{Y - X theta } |_2^2$ . The closed form solution is $ theta^{MLE} = (X^T X)^{-1}X^T Y$ . Maximum A Posteriori . In maximum a posteriori estimation (MAP estimation) we consider $ theta$ a random variable that also follow a normal distribution $ mathcal{N}(0, ,b^{2})$ . We want to maximize $P( theta|X,Y)$ . Using Baye&#39;s theorem we have $$ begin{aligned} P( theta|X,Y) &amp; = frac{P(Y|X, theta)p( theta)}{P(Y|X)} log P( theta|X,Y) &amp; = log P(Y|X, theta) + log p( theta) - log P(Y|X) &amp; = n log 1 - n log sqrt{2 pi sigma^2} - frac{1}{2 sigma^2} |{Y - X theta } |_2^2 + n log 1 - n log sqrt{2 pi b^2} - frac{1}{2 b^2} |{ theta } |_2^2 - log P(Y|X) &amp; = - ( frac{1}{2 sigma^2} |{Y - X theta } |_2^2 + frac{1}{2 b^2} |{ theta } |_2^2 ) + c end{aligned} $$ . Here $c$ is a constant that groups all the terms that do not depend on $ theta$. . From this we can see the cost function for ridge regression $ |{Y - X theta } |_2^2 + lambda |{ theta } |_2^2$ . Ridge regression can be very useful because of its regularizing effect, and the fact that its closed from solution $ theta^{MAP} = (X^T X + lambda I_d)^{-1}X^T Y$ always exists. Also, there is a very useful equality for ridge regression that is discusses here. .",
            "url": "https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/03/05/mle-and-map.html",
            "relUrl": "/notes/ammi/blog/2021/03/05/mle-and-map.html",
            "date": " ‚Ä¢ Mar 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AMMI notes - Polynomial regression and overfitting",
            "content": "Linear regression can be limited if the features are not linearly related with the target. Polynomial regression can help with that. Implementing polynomial regression consists in apply an augmenting function $ Phi$ to the input features. . We could have $ Phi( pmb x) = [x_1,x_2,...,x_d,x_1^2,x_2^2,...,x_d^2]$. . This would square all the features and allow us to learn non linear relationships between the input features and the target. . The solution $ theta = (X^T X)^{-1} X^T Y$ becomes $ theta = ( Phi^T Phi)^{-1} Phi^T Y$ where $ Phi$ is our augmented matrix . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import random . def mse(y_hat,y): return np.mean((y-y_hat)**2) . Elt&#39;s take the function $f(x) = 5x^2 + x$. . n_items = 100 X = np.array(list(range(n_items))).reshape(-1,1) Y = np.array([5*x**2 + x for x,y,z in zip(Xs,Ys,Zs)]).reshape(-1,1) sns.lineplot(x=X[:,0],y=Y[:,0],label=&quot;Function to fit&quot;) plt.ylim((0,50000)) plt.xlim((0,100)) plt.show() . A typical linear regression model will not be able to fit the function $f$. . theta1 = np.linalg.solve(X.T @ X, X.T @ Y) Y_hat = X @ theta1 print(f&quot;MSE: {mse(Y_hat, Y)}&quot;) sns.lineplot(x=X[:,0],y=Y[:,0],label=&quot;Function to fit&quot;) sns.lineplot(x=X[:,0],y=Y_hat[:,0],label=&quot;Function fitted by linear regression&quot;) plt.ylim((0,50000)) plt.xlim((0,100)) plt.show() . MSE: 30470110.138190962 . By augmenting the data we can fit the function $f$. . def augment1(X,degree=2): return np.hstack([X**k for k in range(1,degree+1)]) X2 = augment1(X) theta = np.linalg.solve(X2.T @ X2, X2.T @ Y) Y_hat = X2 @ theta print(f&quot;MSE: {mse(Y_hat, Y)}&quot;) sns.lineplot(x=X[:,0],y=Y[:,0],label=&quot;Function to fit&quot;) sns.lineplot(x=X[:,0],y=Y_hat[:,0],label=&quot;Function fitted by polynomial regression&quot;) plt.ylim((0,20000)) plt.xlim((0,100)) plt.show() . MSE: 1.963163934566787e-23 . It can still work with a relatively high degree for the augmentation. . def augment1(X,degree=2): return np.hstack([X**k for k in range(1,degree+1)]) X2 = augment1(X,degree=5) theta = np.linalg.solve(X2.T @ X2, X2.T @ Y) Y_hat = X2 @ theta print(f&quot;MSE: {mse(Y_hat, Y)}&quot;) sns.lineplot(x=X[:,0],y=Y[:,0],label=&quot;Function to fit&quot;) sns.lineplot(x=X[:,0],y=Y_hat[:,0],label=&quot;Function fitted by polynomial regression&quot;) plt.ylim((0,20000)) plt.xlim((0,100)) plt.show() . MSE: 3.3419740448016697e-21 . However, if the function is too complex, for instance if the degree is too high it can lead to overfitting, especially because of the noise. In this case even with perfect data, the closer the degree gets to the number of data points, the more likely we are to overfit. . def augment1(X,degree=2): return np.hstack([X**k for k in range(1,degree+1)]) X2 = augment1(X,degree=10) theta = np.linalg.solve(X2.T @ X2, X2.T @ Y) Y_hat = X2 @ theta print(f&quot;MSE: {mse(Y_hat, Y)}&quot;) sns.lineplot(x=X[:,0],y=Y[:,0],label=&quot;Function to fit&quot;) sns.lineplot(x=X[:,0],y=Y_hat[:,0],label=&quot;Function fitted by polynomial regression&quot;) plt.ylim((0,20000)) plt.xlim((0,100)) plt.show() . MSE: 5.38062830747916e+33 . Note that we can have more complex functions like $$ Phi( pmb x) = [x_1,x_2,...,x_d, x_1^2,x_1 x_2,...,x_1 x_d,...,x_d^2]$$ . It can be implemeted as follow . def raising(x,degree=2): if degree == 2: return np.outer(x,x)[np.triu_indices(x.shape[0])] return raising(np.outer(x,x)[np.triu_indices(x.shape[0])],degree-1) def augment(X,degree=2): &quot;&quot;&quot; Basis function to generate new features by multiplying existing feature with each other &quot;&quot;&quot; if degree&lt;=1: return X n = X.shape[0] d = X.shape[1] # only values of interest are those of the upper triangular matrix formed # from the outer product of X^(i) by X^(i) tostack = [] for i in range(n): tostack.append(raising(X[i])) B = np.vstack(tostack) return B .",
            "url": "https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/03/04/poynomial-regression-and-overfitting.html",
            "relUrl": "/notes/ammi/blog/2021/03/04/poynomial-regression-and-overfitting.html",
            "date": " ‚Ä¢ Mar 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "AMMI notes - ridge regression",
            "content": "Typically in linear regression, the parameter $ theta$ is obtained using $$ theta = (X^TX)^{-1}X^TY$$ In the case of ridge regression (L2 regularization) however, the expression for $ theta$ is the following: $$ theta = (X^TX + lambda I_d)^{-1}X^TY$$ where d is the number of features Note that with d as the number of features and n the number of examples. The dimensions are as follow: . $ theta$ is (d,1) | $X$ is (n,d) | $Y$ is (n,1) | . Prediction is done using: . $Y = X theta$ | . Ridge regression can be useful for several reasons. It serves as a regularization technique. Moreover, its solution is always defined. The normal equation for the ordinary least sqares requires us to compute the inverse of $X^TX$ which may not exist. The matrix $X^TX + lambda I_d$ however is always inversible. . Showing that $X^TX + lambda I_d$ is always inversible . There are multiple ways of showing that the inverse of $X^TX + lambda I_d$ always exists and oone of those is to show that its determinant cannot be 0. . One of the properties of determinants, for positive semidefinite matrices at least, is: $$det(A + B) geq det(A) + det(B)$$ In our case that means: $$det(X^T X + lambda I_d) geq det(X^T X) + det( lambda I_d)$$ . We need the two matrices to be positive semidefinite for this to work. I will asume $X^T X$ to be positive semidefinite. It&#39;s the Hessian of the loss function we optimize so without it being positive semidefinite, the derivative wouldn&#39;t be convex which would have prevented us from finding the closed form solution the way we did it. In facr, since $X^T X$ is positive semidefinite we have: $det(X^T X) geq 0$ . $$ begin{aligned} det( lambda I_d) &amp; = lambda^d det(I_d) &amp; = lambda^d end{aligned} $$so for $ lambda &gt; 0$ we have $ lambda^d &gt; 0$ and $ lambda I_d$ is postive semidefinite . Since we have $det( lambda I_d) = lambda^d$ and $det(X^T X) geq 0$, we can now say: $$ det(X^T X) + det( lambda I_d) geq lambda^d &gt; 0$$ Therefore $$ det(X^T X + lambda I_d) &gt; 0 $$ and the matrix $X^T X + lambda I_d$ is always inversible . Showing that $(X^TX + lambda I_d)^{-1}X^TY = X^T(XX^T + lambda I_n)^{-1}Y$ . For this one too there are several solutions. My favorite one is from this blog post by Daniel Seita. . Now, when do we use them? I believe it is . $(X^TX + lambda I_d)^{-1}X^TY$ when $n &gt; d$ | $X^T(XX^T + lambda I_n)^{-1}Y$ when $d &gt; n$ | . The reason being that we want the smallest matrix to inverse, since matrix inversion is an expensive operation. . Code . import pandas as pd import numpy as np . !wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv -P data . !ls data . winequality-red.csv . def read_data(path): data = pd.read_csv(path,sep=&quot;;&quot;) X, Y = data[[&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;citric acid&#39;, &#39;residual sugar&#39;, &#39;chlorides&#39;, &#39;free sulfur dioxide&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;]], data[[&quot;quality&quot;]] return X, Y . def split_data(X, Y): split = int(0.8 * len(X)) X_train,Y_train = X[:split].values, Y[:split].values X_test, Y_test = X[split:].values, Y[split:].values return (X_train,Y_train), (X_test, Y_test) . X,Y = read_data(&quot;data/winequality-red.csv&quot;) . (X_train,Y_train), (X_test, Y_test) = split_data(X, Y) . def mse(Y,Y_hat): return np.mean((Y-Y_hat)**2) def LSO(X, Y): &quot;&quot;&quot; Computes ùúÉ using the normal equation &quot;&quot;&quot; theta = np.linalg.inv(X.T @ X) @ X.T @ Y return theta theta1 = LSO(X_train,Y_train) print(f&quot;train error : {mse(X_train @ theta1, Y_train)}&quot;) print(f&quot;test error : {mse(X_test @ theta1, Y_test)}&quot;) . train error : 0.41601754667558516 test error : 0.4315544880376387 . def ridge(X, Y, l=0.1): &quot;&quot;&quot; Computes ùúÉ using ridge regression with the first expression &quot;&quot;&quot; theta = np.linalg.inv(X.T @ X + (l*np.eye(X.shape[1])) ) @ X.T @ Y return theta theta2 = ridge(X_train,Y_train) print(f&quot;train error : {mse(X_train @ theta2, Y_train)}&quot;) print(f&quot;test error : {mse(X_test @ theta2, Y_test)}&quot;) . train error : 0.416193401976357 test error : 0.4321054875487965 . def ridge_2(X, Y, l=0.1): &quot;&quot;&quot; Computes ùúÉ using ridge regression with the second expression &quot;&quot;&quot; theta = X.T @ np.linalg.inv(X @ X.T + (l*np.eye(X.shape[0])) ) @ Y return theta theta3 = ridge_2(X_train,Y_train) print(f&quot;train error : {mse(X_train @ theta3, Y_train)}&quot;) print(f&quot;test error : {mse(X_test @ theta3, Y_test)}&quot;) . train error : 0.41619340197628557 test error : 0.432105487324763 . We can make two observations: . the loss values for the normal regression and the ridge regression are not very different. That&#39;s probably because we are using linear regression so there is not much overfitting. In that case regularization is not very likely to improve performance | The loss values for the two ridge functions are about the same | .",
            "url": "https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/02/20/ridge-equality.html",
            "relUrl": "/notes/ammi/blog/2021/02/20/ridge-equality.html",
            "date": " ‚Ä¢ Feb 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "AMMI notes - Ordinary least square solution",
            "content": "Linear Regression Exercise (Closed Form Solution) . In statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables) [Wikipedia]. The closed form solution to finding the parameter $ theta$ of a linear regression model is given by $$ theta = (X^TX)^{-1}X^TY$$ where $X$ are your features and $Y$ is your target. . Let d be the number of features, n the number of examples. The dimensions are as follow: . $ theta$ is (d,1) | $X$ is (n,d) | $Y$ is (n,1) | . Prediction is done using: . $Y = X theta$ | . We are trying to find the value of $ theta$ that minimizes the squared error which means finding the solution to: $ underset{ theta}{argmin} |{X theta - Y} |_2^2$ . In order to find that value of theta, since the squared error is convex, we can find the derivative of the expression and find the value of $ theta$ that makes it 0. . First let&#39;s expand $ |{X theta - Y} |_2^2$ $$ begin{aligned} |{X theta - Y} |_2^2 &amp;= (X theta - Y)^T(X theta - Y) &amp; = ( theta^T X^T - Y^T)(X theta - Y) &amp; = theta^T X^T X theta - Y^T X theta - theta^T X^T Y - Y^T Y &amp; = theta^T X^T X theta - ( theta^T X^T Y)^T - theta^T X^T Y - Y^T Y &amp; = theta^T X^T X theta - 2 theta^T X^T Y - Y^T Y because theta^T X^T Y is a scalar frac{ partial |{X theta - Y} |_2^2}{ partial theta} &amp; = 2 X^T X theta - 2 X^T Y end{aligned} $$ By equating the derivative to 0 we get: $$ begin{aligned} 2 X^T X theta - 2 X^T Y &amp; = 0 X^T X theta - X^T Y &amp; = 0 X^T X theta &amp; = X^T Y theta &amp; = (X^T X)^{-1} X^T Y end{aligned} $$ . Here is an implementation using numpy and the wine quality dataset from this dataset repo mcu dataset. . import pandas as pd import numpy as np . !wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv -P data . !ls data . winequality-red.csv . data = pd.read_csv(&quot;data/winequality-red.csv&quot;,sep=&#39;;&#39;) data.head(3) . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . cols = [&quot;fixed acidity&quot;, &quot;volatile acidity&quot;, &quot;citric acid&quot;, &quot;residual sugar&quot;, &quot;chlorides&quot;, &quot;free sulfur dioxide&quot;, &quot;total sulfur dioxide&quot;, &quot;density&quot;, &quot;pH&quot;, &quot;sulphates&quot;, &quot;alcohol&quot;] target = &quot;quality&quot; data = data.sample(frac=1) X = data[cols].values Y = data[[target]].values X.shape, Y.shape . ((1599, 11), (1599, 1)) . We also implement the bias parameter by adding a feature with fixed value one to every data point. By doing so we get: $$ sum_{i=1}^{n-1}( theta_i cdot x_i) + theta_n cdot 1 $$ $ theta_n$ will be the bias parameter . def add_ones(X): return np.hstack([X,np.ones((X.shape[0],1))]) class LinearReg: &quot;&quot;&quot; Basic linear regression implemetation using numpy &quot;&quot;&quot; def __init__(self, bias=False): &quot;&quot;&quot; Initialization of theta and a boolean to determine whether to use a bias or not &quot;&quot;&quot; self.theta = None self.bias = bias def fit(self,X,Y): &quot;&quot;&quot; Fit function. Uses the normal equation to compute theta &quot;&quot;&quot; if self.bias: X = add_ones(X) A = X.T @ X B = X.T @ Y self.theta = np.linalg.solve(A,B) #self.theta = np.linalg.inv(A) @ B def predict(self,X): &quot;&quot;&quot; prediction function &quot;&quot;&quot; if self.bias: X = add_ones(X) return X @ self.theta @staticmethod def mse(y_hat,y): &quot;&quot;&quot; Static method implementing the mean squared error &quot;&quot;&quot; return np.mean((y-y_hat)**2) . model1 = LinearReg() model1.fit(X,Y) LinearReg.mse(model1.predict(X),Y) . 0.4170492248204846 . model2 = LinearReg(bias=True) model2.fit(X,Y) LinearReg.mse(model2.predict(X),Y) . 0.41676716722140805 .",
            "url": "https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/02/20/ordinary-least-square-solution.html",
            "relUrl": "/notes/ammi/blog/2021/02/20/ordinary-least-square-solution.html",
            "date": " ‚Ä¢ Feb 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Project DOVE",
            "content": "In December 2019, I participated in a hackathon organized in Ashesi: the HackAPI. This article will explain the solution my team built. . What is project Dove? . ‚ÄúAccording to the biblical story (Genesis 8:11), a dove was released by Noah after the flood in order to find land‚Äù. . https://en.wikipedia.org/wiki/Doves_as_symbols . Similarly, the team I worked with for the 2019 HackAPI decided to send doves on twitter to let us know what is going on. To be a bit more explicit about the product we developed, Dove is a platform that scraps Twitter to find tweets relative to disasters in various locations, mainly floods. . The intended use is mostly information sharing. The motivation for using Twitter is the speed at which information spreads on this particular social network, often preceding traditional news platforms. Dove filters and organizes information from Twitter and displays it in a more digestible way, using a map. . How we built it? . Dove is a web app with three main components: a database, a script for scraping and analyzing text and a frontend. . Database . The database is a simple MongoDB database hosted on mlab.com. It stores information about tweets such as the links, location, the time it was scraped, and the text of the tweet. This information is what the frontend will display to the user. . Frontend . We built the frontend as a single page app with React, which fetches the tweet data directly from the database. This app will then display the information on a map and a feed for the corresponding locations. . Backend script . Then there is the backend script which scraps Twitter for new tweets using pre-defined keywords and locations. For each tweet, the script will use a logistic regression model to determine how likely it is that the tweet is about a disastrous event. If the probability is higher than a pre-defined threshold, the data is sent to the database. . I think it is interesting to note that by tuning the probability threshold, we can decide how much data we are letting in. The search keywords and locations can also be edited to reflect ongoing events. . What does it look like? . To explain what a project is about, talk is cheap, and a video is worth more than a thousand words. If you want to know what the Dove app actually looks like, here is a video where I demo the platform. .",
            "url": "https://jean72human.github.io/ml-blog/blog/hackathon/2020/10/13/project-dove.html",
            "relUrl": "/blog/hackathon/2020/10/13/project-dove.html",
            "date": " ‚Ä¢ Oct 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "My first real team experience in a Zindi competition",
            "content": "I love competitions, especially in data science. After getting started with machine learning, competing on Kaggle and Zindi became one of my favourite hobbies. But between the lack of time, computing resource and experience, I ended my first challenges closer to the bottom of the private leaderboard. Finally, I decided to listen to one excellent advice often given to people starting on Kaggle. I think it was something like this: ‚Äú To win, you need good ensembles; good ensembles are diverse. To get diverse ensembles, you need a good team.‚Äù Based on this advice, I became more open to working in a team. I did, for a Zindi competition and it worked: me and my friend finished 7th which is better than I did in any previous competition. No cash (yet) but a lot of learning. . The team . My first real team experience was in a team of two, with Ronny Polle. If you don‚Äôt know him, he‚Äôs a medical student who likes researching challenging machine learning problems. I met him at the Deep Learning Indaba 2019 in Nairobi. For the Basic Needs Basic Rights - Tech4MentalHealth competition, we formed a team of two. The task consisted of classifying statements from Kenyan university students in terms of the mental health challenges they struggle with. . Overall the experience was quite pleasing and very different from working alone. The aspect I liked the most, apart from the obvious boost on the leaderboard, was the process when coming up with new ideas. A big part of scoring well on such competitions is coming up with new ideas fast and experimenting them. Unfortunately, generating new ideas after seeing the past twenty fail is not always an easy thing to do. When you are not alone, the burden of generating ideas is shared. Even better, your ideas bounce off each other to form even more ideas. So many that it eventually becomes hard to evaluate them all, which was not facilitated by the platform. On Zindi, the daily submission limitations are the same for both teams and individuals, meaning that each individual in a team can make fewer submissions than an individual working solo. These limitations can become frustrating before you even realize it. Despite the pains of having rigid restrictions in terms of submissions, the overall experience was a positive one. . The learning experience . What I learned concerning machine learning . In terms of machine learning, the main thing I learned during this competition is how important cross-validation is. In general, accurately evaluating your models is primordial. Without a proper way of determining the performance of your model, you won‚Äôt be able to compare them, and know what is working and what is not. Apart from that, the challenge helped me confirm a few things: . pretraining is important | when the dataset is small, bigger models do not always help | most of the performance boost comes from tuning hyperparameters The few points mentioned above can seem evident for some, but the quality of the execution of those simple things is what makes a real difference. | . What I learned concerning data science competitions . Relative to competitions, what I learned comes down to this: . ‚ÄúSeed every seedable‚Äù: Zindi will drop you mercilessly if your code does not reproduce the results of your best CSV file, and rightly so. To avoid reproducibility issues, set a fixed value for every random seed that comes to mind. I wish we knew that earlier as the score of our best CSV file is better than the final best score on the private leaderboard. | Keep every notebook: you can never really tell which notebook or code will give you your best score, so keep every single code file. | Experiment ideas in quantity and quality: any method cannot truly be ruled out until we see it fail on the leaderboard, even then, big shake-ups are a thing. Trying as many ideas as possible, then testing them with cross-validation and the leaderboard is the way to go. | . The results . In the end, I was quite satisfied with the results. Teaming up helped me get my first top 10 spot in a data science competition by finishing 7th. I also learned a lot with regards to both machine learning and competitive data science in particular. I didn‚Äôt talk a lot about the final submission, because it was quite basic and was not anything innovative or interesting. The model used was a pre-trained RoBertA model with training and predictions made using 8-fold cross-validation. You can find it on GitHub here .",
            "url": "https://jean72human.github.io/ml-blog/blog/competitions/zindi/2020/09/20/my-first-real-team-experience-in-a-zindi-competition.html",
            "relUrl": "/blog/competitions/zindi/2020/09/20/my-first-real-team-experience-in-a-zindi-competition.html",
            "date": " ‚Ä¢ Sep 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Deep Learning for Video Classification",
            "content": "In my third year of BSc at Ashesi University ( 2019 spring semester), I took a machine learning course. That course was one of my first introduction to the mathematics behind machine learning concepts such as Naive Bayes and deep neural networks. At the end of the semester, in teams of five, we worked on projects that would require us to push ourselves a bit and implement a model that cannot just be imported from a library. My team decided to work on video classification, more specifically, we worked on automatic lip-reading using video inputs. Fast forward to a few weeks ago, the AI Ghana community contacted me. I was asked to present on a machine learning topic of my choice, and I thought it would be an excellent idea to give a talk on video classification using deep learning. . Video . They recorded the presentation and uploaded it to Youtube so here it is: . Github . I also uploaded the code on my Github here. It implemented it using TensorFlow Keras. Note that the dataset used here doesn‚Äôt make any learning possible so do not expect anything when looking at the loss graphs. This code is just here to illustrate how a deep learning model for video classification would look like and how to train it. .",
            "url": "https://jean72human.github.io/ml-blog/blog/2020/08/17/deep-learning-for-video-classification.html",
            "relUrl": "/blog/2020/08/17/deep-learning-for-video-classification.html",
            "date": " ‚Ä¢ Aug 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Multi-source transfer learning and web scraping",
            "content": "Abstract . Machine learning and, more specifically, deep learning have recently driven many innovations. The availability of massive datasets and computation resources has made it possible to create deeper neural networks that are able to learn more meaningful representations of the data. Those new possibilities are not always accessible to the average African company trying to leverage on deep learning to increase profit. In that case, scarcity of data, especially, could be a limitation since neural networks are known to be data-hungry. When faced with the issue of unavailability of public data, a company can either increase the size of the dataset by collecting data themselves or increase the size and complexity of the model. The option studied here is to use web scraping to manage and clean a bigger dataset. In trying to increase the size and complexity of the model, to avoid overfitting, the transfer learning approach was used. This technique involves the transfer of weights from several datasets using model ensembling. All these methods were tested on a rice meal classification problem. The problem consists of classifying images of four rice-based dishes: jollof rice, fried rice, plain rice, and waakye. The dataset contains 60 train images and 20 test images for each group making up a total of 240 training images and 80 testing images. The baseline of 75% was achieved using a dense net Convolutional Neural Network (CNN). The web scraping method used to increase the dataset size attained an accuracy of 87%. A multi-source transfer learning approach was also used where models were pre-trained on the Food-101 dataset and the Food-256 dataset. The multisource transfer learning method achieved an accuracy of 90%. Using these two methods, we implement two ways to significantly increase the efficiency of a model when the original dataset is small. . Introduction . The size of training data plays a vital role in Machine Learning (ML), in learning useful representations that accurately predict the task at hand. Generally, it is common knowledge that a small dataset will result in a sparse approximation of the underlying regularity, resulting in a model with abysmal performance. Techniques have been proposed in ML literature to facilitate selecting the best model in the face of a small training dataset. Methods include employing a short model with fewer parameters, cross-validation 1, transfer learning 2, among others. However, too little data size does not often yield much improvement with these approaches. Most importantly, in the African context where data is often highly unstructured and available in minimal quantities, building models with high accuracy calls for a different approach. This paper, demonstrates how to achieve models with high predictive accuracy, when faced with data that is highly unstructured or unavailable, using Multi-Source Transfer learning (Model Ensembling) or Web Scraping to assemble a structured dataset set for most ML tasks. Multi-Source Transfer is a common practice employed by participants in ML competitions to build winning models, while Web Scraping can be applied to create datasets in a context where no data infrastructure exists to facilitate dataset creation. . Materials and methods . Materials . The following are some of the technologies we used in the project. PyTorch: Pytorch, is a deep learning package, which is known for its high-level tensor computations and building neural networks with less effort. Pytorch is Pythonic, and more importantly, highly optimized for computationally expensive operations, such as convolutional neural networks, recurrence neural networks, and complex tensor operations. Pytorch has many pre-trained models that enhance speedy model training with appreciable high accuracy. This package is still a young player compared to its competitors like TensorFlow. However, it gains momentum very fast due to its features above. Floyd Hub: Training deep learning models is computationally expensive; it requires machines with high processing power. The cheapest way to train the models is to purchase cloud computing services if buying a GPU for your local device is expensive. Floydhub is a cloud computing option employed to train the model under study, because they already preinstalled TensorFlow, PyTorch, Keras, and many more dependencies. Quite apart from having an extensive collection of pre-installed dependencies, Floydhub is simple to use. Google-images-download: This technology is a python package utility for conveniently scraping images from google. However, other powerful web scraping tools exist. . Multi-source transfer learning . Transfer learning consists of transferring knowledge from a more extensive database (source) to a smaller database (target) using weights learned from the bigger database as a starting point when training a model for the second database. The domain of the source and how closely it is related to the domain of the target is also relevant since it can increase the efficiency of the transfer. In this experiment, weights are transfered from several sources, which is known as multi-source transfer learning. One limitation of multi-source transfer learning is that multiple sets of weights cannot be used as a starting point. Hence the use of a model ensembling method to transfer knowledge from all the datasets 3. Multi-Source Transfer or Model Ensembling consists of pooling together the predictions of a set of different models to produce better forecasts ‚Äì where the final model is trained for each of the sources. To fuse the knowledge from these various models, we used an ensemble that concatenates features extracted using the models trained on the sources, and passed the concatenated features to a multilayer perceptron (MLP). The ensemble was then fine-tuned on the target. The models were pre-trained on four datasets: the original dataset, ImageNet, Food101 4, and Food-256 [^5]. . . Web scraping . Web Scraping or web data extraction is used for extracting human-readable data from websites. It is a handy technique used in scenarios where data for a specific ML task is complicated to come by. There are not a lot of data infrastructure that exists in most organizations in the African context. Furthermore, due to bureaucracy and the lack of trust of organizations exposing their data, this technique is powerful for collecting data for various ML tasks in the Africa context. Using this method, two thousand training images of local rice dishes were scrapped in less than thirty (30) minutes. Since a lot of junk images are often downloaded during web scraping, some manual work was done to remove unwanted files and clean up the data into a suitable format. . Implementation details . The methods tested were implemented using PyTorch and run on a virtual machine with a Tesla V100 GPU (16 GB ram) and 8 CPUs. All models were trained with a learning rate of 1e-3. The pre-trained models for Food-101 and Food-256 were trained for five epochs. The models (baselines and ensemble) were trained for 30 epochs. . Results . As a baseline method, a randomly initialized densenet-121 model on the initial dataset was trained. Using this method, an accuracy of 87% was achieved. Table 1 provides a summary of the results obtained. . Method Accuracy . Baseline (randomly initialized densenet-121) | 75% | . Increased dataset | 87% | . Multi-source transfer learning | 90% | . The pre-trained ensemble model achieved the highest accuracy and had a consistently high accuracy across epochs. . . Discussion . After increasing the size of the dataset, with the same method, there was a substantial increase in the level of accuracy. This improvement shows the importance of large data sets, and the limitations of deep learning techniques when dealing with small data. Therefore, for deep learning to be used to its full potential in Africa, it is important that quality local datasets are made public, in order to foster research and make it easier for companies to profit from the deep learning advancement. It also showcases how useful web scraping can be as a data science tool for dataset creation. Multi-source transfer learning resulted in a model that was able to reach high accuracies quite fast. The proposed ensemble also seems not to overfit and shows a stable increase in accuracy. Sharing pre-trained models can greatly help when faced with limited data, especially with pretrained models on various datasets. Our approach could have been made easier to implement if all the pre-trained models were already available. Also, so far this approach would not be suited for all types of inferences because of the big size of the models and their high latency. This can, however, be solved using model distillation and model compression. . Conclusion . In summary, this study showed has how easy it is to obtain a significant improvement in accuracy (up to 15%) over the baseline results, using Multi-Source Transfer Learning. The study also showed how to achieve a significant improvement in accuracy (up to 12%) over the baseline results by employing Web Scraping technique to acquire more dataset for a specific task. There is still a great avenue to improve upon these accuracies. We estimate that we can achieve a very high overall accuracy when these two techniques are combined. In our future work, we hope to test this hypothesis by combining these two techniques. . References . R. Kohavi, ‚ÄúA Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection‚Äù, Research gate, 2001. Available: https://www.researchgate.net/profile/Ron_Kohavi/publication/2352264_A_Study_of_CrossValidation_and_Bootstrap_for_Accuracy_Estimation_and_Model_Selection/links/02e7e51bc c14c5e91c000000.pdf. [Accessed 12 June 2019].¬†&#8617; . | Ng. Hong-Wei &amp; Nguyen, D. Vonikakis, V. Winkler, Stefan. ‚Äú Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning‚Äù. Available: 10.1145/2818346.2830593.¬†&#8617; . | S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe and S. Mougiakakou, ‚ÄúMultisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis‚Äù, IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 76-84, 2017. Available: 10.1109/jbhi.2016.2636929.¬†&#8617; . | L. Bossard, M. Guillaumin and L. Van Gool, ‚ÄúFood-101 ‚Äì Mining Discriminative Components with Random Forests‚Äù, Vision.ee.ethz.ch, 2019. [Online]. Available: https://www.vision.ee.ethz.ch/datasets_extra/food-101/. [Accessed: 13- Jun- 2019]¬†&#8617; . |",
            "url": "https://jean72human.github.io/ml-blog/research/2020/03/20/multi-source-transfer-learning-and-web-scraping.html",
            "relUrl": "/research/2020/03/20/multi-source-transfer-learning-and-web-scraping.html",
            "date": " ‚Ä¢ Mar 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "AMMI notes - MLE and MAP for logistic regression",
            "content": "We did maximum likelihood estimation (MLE} and maximum a posteriori estimation (MAP) in the context of linear regression here. Let&#39;s do the same for classification. . Maximum Likelihood Estimation . Binary classification . Let&#39;s say we have two classes represented by 0 and 1 and we are trying to predict the probability of a feature vector belonging to class 1, we would have: $$ h_ theta (x) = P(y=1|x; theta) 1 - h_ theta (x) = P(y=0|x; theta) $$ . Since we have two possible outcomes (classes) we can use a Bernouilli distribution on the labels and get the likelihood and log likelihood: $$ begin{align} P(y|X; theta) &amp; = h_ theta (x)^y (1 - h_ theta (x))^{1-y} log P(y|X; theta) &amp; = y log h_ theta (x) + (1-y) log (1 - h_ theta (x)) end{align} $$ . Maximizing the log likelihood is the same as minimizing the negative log likelihood. From that we get our loss function to be: $$ mathcal{L}( theta) = - frac{1}{N} sum_{i=1}^{N} y_i log h_ theta (x_i) + (1-y_i) log (1 - h_ theta (x_i)) $$ . Multiclass classification . In the case of multiclass classification we use the multinomial distribution instead of a Bernouilli one. Now, for m classes represented as $[1,2, ... ,m]$, we have $ hat{y} = h_ theta(x)$ to be a vector such that $ hat{y}_k = P(y=k|x; theta)$. . For one data point $(x,y)$ we have: $$ begin{align} P(y|x; theta) &amp; = prod_{k=1}^m frac{ hat{y}_k^{y_k}}{y_k} log P(y|x; theta) &amp; = sum_{k=1}^m log frac{ hat{y}_k^{y_k}}{y_k} &amp; = sum_{k=1}^m log hat{y}_k^{y_k} - sum_{k=1}^m y_k &amp; = sum_{k=1}^m y_k log hat{y}_k - sum_{k=1}^m y_k &amp; = sum_{k=1}^m y_k log hat{y}_k + c end{align} $$ . Here $c$ is a constant not depending on $ theta$ so it is not part of the loss function. . We get our loss function to be: $$ mathcal{L}( theta) = - frac{1}{N} sum_{i=1}^N sum_{k=1}^m y_k log h_ theta(x)_k $$ . Maximum a posteriori estimation . The MAP for logistic regression and linear regression are very similar. We consider $ theta$ a random variable that follows a normal distribution $ mathcal{N}(0, ,b^{2})$. We want to maximize $P( theta|X,Y)$ . Using Baye&#39;s theorem we have $$ begin{aligned} P( theta|X,Y) &amp; = frac{P(Y|X, theta)p( theta)}{P(Y|X)} log P( theta|X,Y) &amp; = log P(Y|X, theta) + log p( theta) - log P(Y|X) &amp; = y log h_ theta (x) + (1-y) log (1 - h_ theta (x)) - n log sqrt{2 pi b^2} - frac{1}{2 b^2} |{ theta } |_2^2 - log P(Y|X) &amp; = (y log h_ theta (x) + (1-y) log (1 - h_ theta (x)) - frac{1}{2 b^2} |{ theta } |_2^2 ) + c end{aligned} $$ . Here $c$ is a constant that groups all the terms that do not depend on $ theta$. . From that we get our loss function to be: $$ mathcal{L}( theta) = -( frac{1}{N} sum_{i=1}^{N} y_i log h_ theta (x_i) + (1-y_i) log (1 - h_ theta (x_i)) + lambda | theta |_2^2) $$ .",
            "url": "https://jean72human.github.io/ml-blog/notes/ammi/blog/2020/03/10/mle-and-map-logistic-regression.html",
            "relUrl": "/notes/ammi/blog/2020/03/10/mle-and-map-logistic-regression.html",
            "date": " ‚Ä¢ Mar 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Blog",
          "content": "Blog . Here are my blog articles that are not related to any research projects in particular. I will post about side projects here, as well as my learning experiences, technologies I fall in love with and data science competitions I enter. . AMMI notes - MLE and MAP . maximum likelihood estimation and maximum a posteriori estimation . Mar 5, 2021 . | AMMI notes - Polynomial regression and overfitting . Exploring the limits of linear regression and how polynomial regression gives better results in some cases . Mar 4, 2021 . | AMMI notes - ridge regression . An explanation of a useful equality for the ridge regression . Feb 20, 2021 . | AMMI notes - Ordinary least square solution . Solution to the optimization of the squarred error with accompanying code. . Feb 20, 2021 . | Project DOVE . Project DOVE was built for the 2020 HackAPI. It&#39;s a web app that scraps Twitter for news, filters it and displays it. . Oct 13, 2020 . | My first real team experience in a Zindi competition . A description of my first real team experience in a Zindi competition with Ronny, a friend of mine. . Sep 20, 2020 . | Deep Learning for Video Classification . A presentation made to the AI Ghana community on the use of deep learning for video classification. . Aug 17, 2020 . | AMMI notes - MLE and MAP for logistic regression . maximum likelihood estimation and maximum a posteriori estimation for logistic regression . Mar 10, 2020 . | .",
          "url": "https://jean72human.github.io/ml-blog/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Research",
          "content": "Research projects . These are my (mini) research projects. Mostly projects that are technical, took time (several months) and effort will be posted here. Some are projects I did in teams, some I did alone. Hopefully, one day I publish them. . Multi-source transfer learning and web scraping . Employing multi-source transfer learning and web scraping to increase model accuracy when the dataset is limited. . Mar 20, 2020 . | .",
          "url": "https://jean72human.github.io/ml-blog/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://jean72human.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}