  
---
layout: default
---
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">AMMI notes - MLE and MAP</h1><p class="page-description">maximum likelihood estimation and maximum a posteriori estimation</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-05T00:00:00-06:00" itemprop="datePublished">
        Mar 5, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Gbetondji Dovonon</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ml-blog/categories/#notes">notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#ammi">ammi</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#blog">blog</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/jean72human/ml-blog/tree/master/_notebooks/2021-03-05-mle-and-map.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/ml-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/jean72human/ml-blog/master?filepath=_notebooks%2F2021-03-05-mle-and-map.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/jean72human/ml-blog/blob/master/_notebooks/2021-03-05-mle-and-map.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Maximum-Likelihood-Estimation">Maximum Likelihood Estimation </a></li>
<li class="toc-entry toc-h2"><a href="#Maximum-A-Posteriori">Maximum A Posteriori </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-05-mle-and-map.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear regression works by finding $\underset{\theta}{argmin} \|{X\theta - Y}\|_2^2$. \
The interpretations for the metric we are minimizing are diverse. It can be interpreted as the sum of the euclidian distances between the predictions and actual values. It can also be obtained by maximum likelihood estimation (MLE).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-Likelihood-Estimation">
<a class="anchor" href="#Maximum-Likelihood-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood Estimation<a class="anchor-link" href="#Maximum-Likelihood-Estimation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In MLE we are interested in finding the value of theta that maximizes the likelihood of the data as expressed by $p(Y|X\theta)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's consider every data point to be of the form $y_i = x_i \theta + \epsilon$ where $\epsilon$ follows a normal distribution $\mathcal{N}(0,\,\sigma^{2})$ and $y_i$ will follow a distribution $\mathcal{N}(x_i \theta,\,\sigma^{2})$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our goal is now to find $\underset{\theta}{argmax}\ p(Y|X,\theta)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because we assume that the data points are independently sampled we can write $p(Y|X,\theta) = \displaystyle\prod_{i=1}^{n} P(y_i|x_i,\theta) $</p>
<p>Then because we assume the data points are identically distributed, in this case with a distribution $\mathcal{N}(x_i \theta,\,\sigma^{2})$, we can write $\displaystyle\prod_{i=1}^{n} P(y_i|x_i,\theta) = \displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(y_i - x_i \theta)^2}{2 \sigma^2} $</p>
<p>Lastly, because the $\log$ function is a strictly increasing function, maximizing the likelihood $p(Y|X,\theta)$ is the same as maximizing the log likelihood $\log p(Y|X,\theta)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using that let's develop the expression of the likelihood.
$$
\begin{aligned}
p(Y|X,\theta) &amp; = \displaystyle\prod_{i=1}^{n} P(y_i|x_i,\theta) 
\\
&amp; = \displaystyle\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(y_i - x_i \theta)^2}{2 \sigma^2} )
\\
\log p(Y|X,\theta) &amp; = \displaystyle\sum_{i=1}^{n} \log ( \frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(y_i - x_i \theta)^2}{2 \sigma^2} ))
\\
&amp; = \displaystyle\sum_{i=1}^{n} \log \frac{1}{\sqrt{2 \pi \sigma^2}} - \displaystyle\sum_{i=1}^{n} \frac{(y_i - x_i \theta)^2}{2 \sigma^2}  
\\
&amp; = n \log 1 - n \log \sqrt{2 \pi \sigma^2} - \frac{1}{2 \sigma^2} \displaystyle\sum_{i=1}^{n} (y_i - x_i \theta)^2 
\\
&amp; = n \log 1 - n \log \sqrt{2 \pi \sigma^2} - \frac{1}{2 \sigma^2} \|{Y - X\theta }\|_2^2
\end{aligned}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From that we can see that fincing the value of $\theta$ that maximizes the likelihood is the same as finding the $\theta$ that minimizes $\|{Y - X\theta }\|_2^2$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The closed form solution is $\theta^{MLE} = (X^T X)^{-1}X^T Y$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-A-Posteriori">
<a class="anchor" href="#Maximum-A-Posteriori" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum A Posteriori<a class="anchor-link" href="#Maximum-A-Posteriori"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In maximum a posteriori estimation (MAP estimation) we consider $\theta$ a random variable that also follow a normal distribution $\mathcal{N}(0,\,b^{2})$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We want to maximize $P(\theta|X,Y)$</p>
<p>Using Baye's theorem we have 
$$
\begin{aligned}
P(\theta|X,Y) &amp; = \frac{P(Y|X,\theta)p(\theta)}{P(Y|X)}
\\
\ log P(\theta|X,Y) &amp; = \log P(Y|X,\theta) + \log p(\theta) - \log P(Y|X)
\\
&amp; = n \log 1 - n \log \sqrt{2 \pi \sigma^2} - \frac{1}{2 \sigma^2} \|{Y - X\theta }\|_2^2 + n \log 1 - n \log \sqrt{2 \pi b^2} - \frac{1}{2 b^2} \|{\theta }\|_2^2 - \log P(Y|X)
\\
&amp; = - (\frac{1}{2 \sigma^2} \|{Y - X\theta }\|_2^2 + \frac{1}{2 b^2} \|{\theta }\|_2^2 ) + c
\end{aligned}
$$</p>
<p>Here $c$ is a constant that groups all the terms that do not depend on $\theta$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From this we can see the cost function for ridge regression $\|{Y - X\theta }\|_2^2 + \lambda \|{\theta }\|_2^2$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ridge regression can be very useful because of its regularizing effect, and the fact that its closed from solution $\theta^{MAP} = (X^T X + \lambda I_d)^{-1}X^T Y$ always exists. Also, there is a very useful equality for ridge regression that is discusses <a href="https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/02/20/ridge-equality.html">here</a>.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jean72human/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ml-blog/notes/ammi/blog/2021/03/05/mle-and-map.html" hidden></a>
</article>