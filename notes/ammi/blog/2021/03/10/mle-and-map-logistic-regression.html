  
---
layout: default
---
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">AMMI notes - MLE and MAP for logistic regression</h1><p class="page-description">maximum likelihood estimation and maximum a posteriori estimation for logistic regression</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-10T00:00:00-06:00" itemprop="datePublished">
        Mar 10, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Gbetondji Dovonon, Sebenele Thwala, Stephen Oni</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ml-blog/categories/#notes">notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#ammi">ammi</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#blog">blog</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/jean72human/ml-blog/tree/master/_notebooks/2021-03-10-mle-and-map-logistic-regression.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/ml-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/jean72human/ml-blog/master?filepath=_notebooks%2F2021-03-10-mle-and-map-logistic-regression.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/jean72human/ml-blog/blob/master/_notebooks/2021-03-10-mle-and-map-logistic-regression.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Maximum-Likelihood-Estimation">Maximum Likelihood Estimation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Binary-classification">Binary classification </a></li>
<li class="toc-entry toc-h3"><a href="#Multiclass-classification">Multiclass classification </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Maximum-a-posteriori-estimation">Maximum a posteriori estimation </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-10-mle-and-map-logistic-regression.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We did maximum likelihood estimation (MLE} and maximum a posteriori estimation (MAP) in the context of linear regression <a href="https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/03/05/mle-and-map.html">here</a>. 
Let's do the same for classification.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-Likelihood-Estimation">
<a class="anchor" href="#Maximum-Likelihood-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood Estimation<a class="anchor-link" href="#Maximum-Likelihood-Estimation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Binary-classification">
<a class="anchor" href="#Binary-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Binary classification<a class="anchor-link" href="#Binary-classification"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's say we have two classes represented by 0 and 1 and we are trying to predict the probability of a feature vector belonging to class 1, we would have:
$$
h_\theta (x) = P(y=1|x;\theta)
\\
1 - h_\theta (x) = P(y=0|x;\theta)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we have two possible outcomes (classes) we can use a Bernouilli distribution on the labels and get the likelihood and log likelihood:
$$
\begin{aligned}
P(y|X;\theta) &amp; = h_\theta (x)^y (1 - h_\theta (x))^{1-y}
\\
\log P(y|X;\theta) &amp; = y \log h_\theta (x) + (1-y) \log (1 - h_\theta (x))
\end{aligned}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Maximizing the log likelihood is the same as minimizing the negative log likelihood. From that we get our loss function to be:
$$
\mathcal{L}(\theta) = -\frac{1}{N}\sum_{i=1}^{N} y_i \log h_\theta (x_i) + (1-y_i) \log (1 - h_\theta (x_i))
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiclass-classification">
<a class="anchor" href="#Multiclass-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiclass classification<a class="anchor-link" href="#Multiclass-classification"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case of multiclass classification we use the multinomial distribution instead of a Bernouilli one. Now, for m classes represented as $[1,2, ... ,m]$, we have $\hat{y} = h_\theta(x)$ to be a vector such that $\hat{y}_k = P(y=k|x;\theta)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For one data point $(x,y)$ we have:
$$
\begin{aligned}
P(y|x;\theta) &amp; = \prod_{k=1}^m \frac{\hat{y}_k^{y_k}}{y_k}
\\
\log P(y|x;\theta) &amp; = \sum_{k=1}^m \log \frac{\hat{y}_k^{y_k}}{y_k}
\\
&amp; = \sum_{k=1}^m \log \hat{y}_k^{y_k} - \sum_{k=1}^m \log y_k
\\
&amp; = \sum_{k=1}^m y_k \log \hat{y}_k - \sum_{k=1}^m \log y_k
\\
&amp; = \sum_{k=1}^m y_k \log \hat{y}_k + c
\end{aligned}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here $c$ is a constant not depending on $\theta$ so it is not part of the loss function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We get our loss function to be:
$$
\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^m y_k \log h_\theta(x)_k
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-a-posteriori-estimation">
<a class="anchor" href="#Maximum-a-posteriori-estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum a posteriori estimation<a class="anchor-link" href="#Maximum-a-posteriori-estimation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The MAP for logistic regression and linear regression are very similar. We consider $\theta$ a random variable that follows a normal distribution $\mathcal{N}(0,\,b^{2})$. We want to maximize $P(\theta|X,Y)$</p>
<p>Using Baye's theorem we have 
$$
\begin{aligned}
P(\theta|X,Y) &amp; = \frac{P(Y|X,\theta)p(\theta)}{P(Y|X)}
\\
\ log P(\theta|X,Y) &amp; = \log P(Y|X,\theta) + \log p(\theta) - \log P(Y|X)
\\
&amp; = y \log h_\theta (x) + (1-y) \log (1 - h_\theta (x)) - n \log \sqrt{2 \pi b^2} - \frac{1}{2 b^2} \|{\theta }\|_2^2 - \log P(Y|X)
\\
&amp; = (y \log h_\theta (x) + (1-y) \log (1 - h_\theta (x)) - \frac{1}{2 b^2} \|{\theta }\|_2^2 ) + c
\end{aligned}
$$</p>
<p>Here $c$ is a constant that groups all the terms that do not depend on $\theta$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From that we get our loss function to be:
$$
\mathcal{L}(\theta) = -(\frac{1}{N}\sum_{i=1}^{N} y_i \log h_\theta (x_i) + (1-y_i) \log (1 - h_\theta (x_i)) + \lambda \| \theta \|_2^2)
$$</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jean72human/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ml-blog/notes/ammi/blog/2021/03/10/mle-and-map-logistic-regression.html" hidden></a>
</article>