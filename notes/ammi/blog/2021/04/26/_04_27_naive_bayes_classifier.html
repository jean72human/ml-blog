<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>AMMI notes - Naive Bayes classifier for sentiment analysis | Gbetondji Dovonon</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="AMMI notes - Naive Bayes classifier for sentiment analysis" />
<meta name="author" content="Gbetondji Dovonon" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using a Naive Bayes classifier to classify reviews" />
<meta property="og:description" content="Using a Naive Bayes classifier to classify reviews" />
<link rel="canonical" href="https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/04/26/_04_27_naive_bayes_classifier.html" />
<meta property="og:url" content="https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/04/26/_04_27_naive_bayes_classifier.html" />
<meta property="og:site_name" content="Gbetondji Dovonon" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-26T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Gbetondji Dovonon"},"description":"Using a Naive Bayes classifier to classify reviews","@type":"BlogPosting","headline":"AMMI notes - Naive Bayes classifier for sentiment analysis","dateModified":"2021-04-26T00:00:00-05:00","datePublished":"2021-04-26T00:00:00-05:00","url":"https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/04/26/_04_27_naive_bayes_classifier.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/04/26/_04_27_naive_bayes_classifier.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ml-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jean72human.github.io/ml-blog/feed.xml" title="Gbetondji Dovonon" /><link rel="shortcut icon" type="image/x-icon" href="/ml-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ml-blog/">Gbetondji Dovonon</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ml-blog/blog/">Blog</a><a class="page-link" href="/ml-blog/research/">Research</a><a class="page-link" href="/ml-blog/search/">Search</a><a class="page-link" href="/ml-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">AMMI notes - Naive Bayes classifier for sentiment analysis</h1><p class="page-description">Using a Naive Bayes classifier to classify reviews</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-04-26T00:00:00-05:00" itemprop="datePublished">
        Apr 26, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Gbetondji Dovonon</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ml-blog/categories/#notes">notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#ammi">ammi</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ml-blog/categories/#blog">blog</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/jean72human/ml-blog/tree/master/_notebooks/2021_04_27_naive_bayes_classifier.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/ml-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/jean72human/ml-blog/master?filepath=_notebooks%2F2021_04_27_naive_bayes_classifier.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/jean72human/ml-blog/blob/master/_notebooks/2021_04_27_naive_bayes_classifier.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ml-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021_04_27_naive_bayes_classifier.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a simple implementation of Naive Bayes classifier for review rating. The classifier uses Baye's rule to determine the probability of a text belonging to a certain class, then we find the class with the highest probability.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's not a token belonging to a sentence as $w_i$ where $i$ is its position in the sentence. We can find the probability $P(c | w_1 w_2 ... w_n)$ for a piece of text with $n$ tokens. Using Baye's rule, and assuming independence of the tokens with respect to each other, we have
$$
\begin{aligned}
P(c | w_1 w_2 ... w_n) &amp; \propto P(c) P( w_1 w_2 ... w_n | c) \\
&amp; \propto P(c) P( w_1 | c) P (w_2 | c) ... P( w_n | c) \\
&amp; \propto P(c) \prod_{i=0}^{n} P( w_i | c) \\
&amp; \propto \log P(c) + \sum_{i=0}^{n} \log P( w_i | c) 
\end{aligned}
$$</p>
<p>This model doesn't directly use the probability $P(c | w_1 w_2 ... w_n)$ but rather uses the likelihood $P( w_1 w_2 ... w_n | c)$ and prior $P(c)$. It therefore belongs to the family of generative models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use counts to approximate those probabilties.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
P(w_i | c) = \frac{count(w_i)}{count(c)}
$$<p>with $count(w_i)$ being the number of times the token $w_i$ appears in class $c$ and $count(c)$ the number of examples in that class.
$$
P(c) = \frac{count(c)}{|\mathcal{D}|}
$$
with $count(c)$ the number of examples in that class and $|\mathcal{D}|$ the size of the dataset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will try the classifier on review data. We will combine labeled review data from Amazon, Imdb and Yelp.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will use the nltk python package for preprocessing. More specifically, it will be the punkt tokenizer and wordnet lemmatizer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'punkt'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'wordnet'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we have the code for the class. The notable methods here are:</p>
<ul>
<li>
<code>_train</code>: it collects the various statistics we need for our classifier</li>
<li>
<code>_predict</code>: uses those statisctics to make predictions</li>
<li>
<code>preprocess</code>: takes in the sentence and does the tokenization, normalization and lemmatization</li>
</ul>
<p>I also added a method to load and save the model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is the initializer for our naive classifier</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">naive_classifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
            <span class="sd">"""</span>
<span class="sd">            Initialization of the </span>
<span class="sd">            """</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">likelihoods</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span> <span class="p">:</span> <span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span><span class="p">)</span> <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">priors</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
<p>It only takes <code>classes</code> as argument. That would be a list of classes to predict like <code>["postive","negative]</code> for instance. 
There is a <code>trained</code> boolean that will be set to true once training is done. <code>n_classes</code> is the number of classes. The <code>likelihoods</code> variable will be a dictionary that will contain as many dictionaries as there are classes. The dictionaries inside that pariable will contain the probabilities of the words belonging to that class, $P( w_i | c) $ in the equations above. It would look somehow like this:</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">"positive"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"bad"</span><span class="p">:</span> <span class="mf">0.007</span><span class="p">,</span> <span class="c1"># probability of the word 'bad' being in a postive review</span>
        <span class="s2">"great"</span><span class="p">:</span> <span class="mf">0.401</span><span class="p">,</span> <span class="c1"># probability of the word 'great' being in a postive review</span>
        <span class="o">...</span>
        <span class="s2">"good"</span><span class="p">:</span> <span class="mf">0.576</span><span class="p">,</span> <span class="c1"># probability of the word 'good' being in a postive review</span>
    <span class="p">},</span>
    <span class="s2">"negative"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"bad"</span><span class="p">:</span> <span class="mf">0.72</span><span class="p">,</span> <span class="c1"># probability of the word 'bad' being in a negative review</span>
        <span class="s2">"great"</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="c1"># probability of the word 'great' being in a negative review</span>
        <span class="o">...</span>
        <span class="s2">"good"</span><span class="p">:</span> <span class="mf">0.016</span><span class="p">,</span> <span class="c1"># probability of the word 'good' being in a negative review</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>There's a <code>prior</code> for the probabilities of each class, $P(c)$ in the equations above.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I will skip the read method. You can check it out by running the notebook. Just go to the top of the page and click on "open in colab". You can also use "view on github".</p>
<p>The first method we are interested in is the preprocessing method.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    preprocesses the sentences. Tokenizes them, then lemmatizes the token using the wordnet lemmatizer</span>
<span class="sd">    """</span>
    <span class="kn">import</span> <span class="nn">string</span>
    <span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
    <span class="n">wordnet_lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
    <span class="n">toReturn</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">):</span>
            <span class="n">toReturn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordnet_lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">toReturn</span>
</pre></div>
<p>It takes in a string object. NLTK provides us with a tokenizer, it will break the sentence into a list of tokens. We will then replace each token by its lemma using the wordnet lemmatizer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at the training function:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">):</span>
    <span class="n">classCounts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span><span class="p">)]</span>
    <span class="n">ndoc</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="n">wordCounts</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span> <span class="p">:</span> <span class="nb">dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span><span class="p">)}</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="n">review</span> <span class="o">=</span> <span class="n">document</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">document</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">classCounts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">review</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">wordCounts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">wordCounts</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">wordCounts</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">classCounts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">/</span><span class="n">ndoc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">"Vocabulary size: "</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
                <span class="n">numerator</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> 
                <span class="n">denominator</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">likelihoods</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">## for words that are not yet in the vocabulary, we start from one</span>
                <span class="n">numerator</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">denominator</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">wordCounts</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">likelihoods</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span><span class="p">)</span>
</pre></div>
<p>The first for loop of this code collects the counts in a dictionary called <code>wordCounts</code>. The second one computes the priors for each class and creates the vocabulary. The vocabulary is the combined set of tokens from all the classes.  The last loop then uses the compiled statistics to compute the likelihoods for each word within each class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's look at the predict function.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Takes tokenized input and outputs numerical class</span>
<span class="sd">    """</span>
    <span class="kn">import</span> <span class="nn">operator</span>
    <span class="n">sumc</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nclasses</span><span class="p">):</span>
        <span class="n">sumc</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">:</span>
                <span class="n">sumc</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihoods</span><span class="p">[</span><span class="n">c</span><span class="p">][</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">sumc</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
<p>In this method, we loop trough the classes and compute the value of $\log P(c) + \sum_{i=0}^{n} \log P( w_i | c)$. We put them in a variable called sumc which is a dictionary with the classes as keys. We first add the class prior, then we add the likelihoods of each token for that specific class. 
THe last line then gives us the key with the maximum value, that is the predicted class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We train and test the classifier. We can see that even a naive classifier can achieve a relatively good accuracy on simple problems.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">naive_classifier</span><span class="p">(</span><span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"positive"</span><span class="p">,</span> <span class="s2">"negative"</span><span class="p">])</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">([</span><span class="s2">"./amazon_cells_labelled.txt"</span><span class="p">,</span>
                  <span class="s2">"./imdb_labelled.txt"</span><span class="p">,</span>
                  <span class="s2">"./yelp_labelled.txt"</span><span class="p">],</span>
                 <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">split_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>reading:  ./amazon_cells_labelled.txt
reading:  ./imdb_labelled.txt
reading:  ./yelp_labelled.txt
Vocabulary size:  4371
2400  training items
600  testing items
Training done
Train accuracy:  0.93875
Test accuracy:  0.83
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now test the classifier with any data at all and see the result. Here is an example that belongs to the positive class.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_text1</span> <span class="o">=</span> <span class="s2">"Mushoku Tensei is the greatest light novel series ever! It has great world building, foreshadowing and character development"</span>

<span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_text1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is one that belongs to the negative class</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_text2</span> <span class="o">=</span> <span class="s2">"The new iPhone was not satisfactory. The innovation is not there anymore and prices are still through the roof"</span>

<span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_text2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jean72human/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ml-blog/notes/ammi/blog/2021/04/26/_04_27_naive_bayes_classifier.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ml-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ml-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ml-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Aspiring machine learning researcher</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jean72human" title="jean72human"><svg class="svg-icon grey"><use xlink:href="/ml-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jean72human" title="jean72human"><svg class="svg-icon grey"><use xlink:href="/ml-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
