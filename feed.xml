<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jean72human.github.io/ml-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jean72human.github.io/ml-blog/" rel="alternate" type="text/html" /><updated>2021-02-22T10:26:57-06:00</updated><id>https://jean72human.github.io/ml-blog/feed.xml</id><title type="html">Gbetondji Dovonon</title><subtitle>Aspiring machine learning researcher</subtitle><entry><title type="html">Title</title><link href="https://jean72human.github.io/ml-blog/2021/02/22/ordinary-least-squares-solution.html" rel="alternate" type="text/html" title="Title" /><published>2021-02-22T00:00:00-06:00</published><updated>2021-02-22T00:00:00-06:00</updated><id>https://jean72human.github.io/ml-blog/2021/02/22/ordinary-least-squares-solution</id><content type="html" xml:base="https://jean72human.github.io/ml-blog/2021/02/22/ordinary-least-squares-solution.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-22-ordinary-least-squares-solution.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;AMMI-notes---squarred-error-solution&quot;&gt;AMMI notes - squarred error solution&lt;a class=&quot;anchor-link&quot; href=&quot;#AMMI-notes---squarred-error-solution&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;Solution to the optimization of the squarred error with accompanying code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;toc:true - badges: true&lt;/li&gt;
&lt;li&gt;comments: true&lt;/li&gt;
&lt;li&gt;sticky_rank: 1&lt;/li&gt;
&lt;li&gt;author: Gbetondji Dovonon&lt;/li&gt;
&lt;li&gt;categories: [notes,ammi]&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Linear Regression Exercise (Closed Form Solution)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables) [Wikipedia]. The closed form solution to finding the parameter $\theta$ of a linear regression model is given by $$\theta = (X^TX)^{-1}X^TY$$ where $X$ are your features and $Y$ is your target.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let d be the number of features, n the number of examples.
The dimensions are as follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta$ is (d,1)&lt;/li&gt;
&lt;li&gt;$X$ is (n,f)&lt;/li&gt;
&lt;li&gt;$Y$ is (n,1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prediction is done using:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y = X \theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are trying to find the value of $\theta$ that minimizes the squared error which means finding the solution to: \
$\underset{\theta}{argmin} \|{X \theta - Y}\|_2^2$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In order to find that value of theta, since the squared error is convex, we can find the derivative of the expression and find the value of $\theta$ that makes it 0.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;First let's expand $\|{X \theta - Y}\|_2^2$
\begin{split}
\\
\\
\|{X \theta - Y}\|_2^2 &amp;amp;= (X \theta - Y)^T(X \theta - Y) \\
&amp;amp; = (\theta^T X^T  - Y^T)(X \theta - Y) \\
&amp;amp; = \theta^T X^T X \theta - Y^T X \theta - \theta^T X^T Y - Y^T Y \\
&amp;amp; = \theta^T X^T X \theta - (\theta^T X^T Y)^T - \theta^T X^T Y - Y^T Y \\ 
&amp;amp; = \theta^T X^T X \theta - 2\theta^T X^T Y - Y^T Y \ because \ \theta^T X^T Y \ is \ a \ scalar \\
\\
\\
\frac{\partial \|{X \theta - Y}\|_2^2}{\partial \theta} &amp;amp; = 2 X^T X \theta -  2 X^T Y \\
\\
\\
\end{split}
By equating the derivative to 0 we get:&lt;/p&gt;
\begin{split}
2 X^T X \theta -  2 X^T Y &amp;amp; = 0 \\
X^T X \theta -  X^T Y &amp;amp; = 0 \\
X^T X \theta &amp;amp; =  X^T Y \\
\theta &amp;amp; = (X^T X)^{-1} X^T Y \\
\end{split}
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is an implementation using numpy and the wine quality dataset from this dataset repo &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets.php&quot;&gt;mcu dataset&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;PART B&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv -P data
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;zsh:1: command not found: wget
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;ls data
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;winequality-red.csv
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data/winequality-red.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;fixed acidity&lt;/th&gt;
      &lt;th&gt;volatile acidity&lt;/th&gt;
      &lt;th&gt;citric acid&lt;/th&gt;
      &lt;th&gt;residual sugar&lt;/th&gt;
      &lt;th&gt;chlorides&lt;/th&gt;
      &lt;th&gt;free sulfur dioxide&lt;/th&gt;
      &lt;th&gt;total sulfur dioxide&lt;/th&gt;
      &lt;th&gt;density&lt;/th&gt;
      &lt;th&gt;pH&lt;/th&gt;
      &lt;th&gt;sulphates&lt;/th&gt;
      &lt;th&gt;alcohol&lt;/th&gt;
      &lt;th&gt;quality&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;7.4&lt;/td&gt;
      &lt;td&gt;0.70&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;1.9&lt;/td&gt;
      &lt;td&gt;0.076&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;34.0&lt;/td&gt;
      &lt;td&gt;0.9978&lt;/td&gt;
      &lt;td&gt;3.51&lt;/td&gt;
      &lt;td&gt;0.56&lt;/td&gt;
      &lt;td&gt;9.4&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;7.8&lt;/td&gt;
      &lt;td&gt;0.88&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;0.098&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;67.0&lt;/td&gt;
      &lt;td&gt;0.9968&lt;/td&gt;
      &lt;td&gt;3.20&lt;/td&gt;
      &lt;td&gt;0.68&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;7.8&lt;/td&gt;
      &lt;td&gt;0.76&lt;/td&gt;
      &lt;td&gt;0.04&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;0.092&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;0.9970&lt;/td&gt;
      &lt;td&gt;3.26&lt;/td&gt;
      &lt;td&gt;0.65&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;fixed acidity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;volatile acidity&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;citric acid&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;residual sugar&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;chlorides&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;free sulfur dioxide&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;total sulfur dioxide&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;density&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;pH&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;sulphates&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;alcohol&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;quality&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frac&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((1599, 11), (1599, 1))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add_ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearReg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Basic linear regression implemetation using numpy&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Initialization of theta and a boolean to determine whether to use a bias or not&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
  
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Fit function. Uses the normal equation to compute theta&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#self.theta = np.linalg.inv(A) @ B&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        prediction function&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@staticmethod&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        Static method implementing the mean squared error&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearReg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LinearReg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.4170492248204846&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearReg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LinearReg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.41676716722140805&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Project DOVE</title><link href="https://jean72human.github.io/ml-blog/blog/hackathon/2020/10/13/project-dove.html" rel="alternate" type="text/html" title="Project DOVE" /><published>2020-10-13T00:00:00-05:00</published><updated>2020-10-13T00:00:00-05:00</updated><id>https://jean72human.github.io/ml-blog/blog/hackathon/2020/10/13/project-dove</id><content type="html" xml:base="https://jean72human.github.io/ml-blog/blog/hackathon/2020/10/13/project-dove.html">&lt;p&gt;In December 2019, I participated in a hackathon organized in Ashesi: the &lt;a href=&quot;https://twitter.com/hack_api&quot;&gt;HackAPI&lt;/a&gt;. This article will explain the solution my team built.&lt;/p&gt;

&lt;h2 id=&quot;what-is-project-dove&quot;&gt;What is project Dove?&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;“According to the biblical story (Genesis 8:11), a dove was released by Noah after the flood in order to find land”.&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;https://en.wikipedia.org/wiki/Doves_as_symbols&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Similarly, the team I worked with for the 2019 HackAPI decided to send doves on twitter to let us know what is going on. To be a bit more explicit about the product we developed, Dove is a platform that scraps Twitter to find tweets relative to disasters in various locations, mainly floods.&lt;/p&gt;

&lt;p&gt;The intended use is mostly information sharing. The motivation for using Twitter is the speed at which information spreads on this particular social network, often preceding traditional news platforms. Dove filters and organizes information from Twitter and displays it in a more digestible way, using a map.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-it&quot;&gt;How we built it?&lt;/h2&gt;
&lt;p&gt;Dove is a web app with three main components: a database, a script for scraping and analyzing text and a frontend.&lt;/p&gt;
&lt;h3 id=&quot;database&quot;&gt;Database&lt;/h3&gt;
&lt;p&gt;The database is a simple MongoDB database hosted on mlab.com. It stores information about tweets such as the links, location, the time it was scraped, and the text of the tweet. This information is what the frontend will display to the user.&lt;/p&gt;
&lt;h3 id=&quot;frontend&quot;&gt;Frontend&lt;/h3&gt;
&lt;p&gt;We built the frontend as a single page app with React, which fetches the tweet data directly from the database. This app will then display the information on a map and a feed for the corresponding locations.&lt;/p&gt;
&lt;h3 id=&quot;backend-script&quot;&gt;Backend script&lt;/h3&gt;
&lt;p&gt;Then there is the backend script which scraps Twitter for new tweets using pre-defined keywords and locations. For each tweet, the script will use a logistic regression model to determine how likely it is that the tweet is about a disastrous event. If the probability is higher than a pre-defined threshold, the data is sent to the database.&lt;/p&gt;

&lt;p&gt;I think it is interesting to note that by tuning the probability threshold, we can decide how much data we are letting in. The search keywords and locations can also be edited to reflect ongoing events.&lt;/p&gt;

&lt;h2 id=&quot;what-does-it-look-like&quot;&gt;What does it look like?&lt;/h2&gt;
&lt;p&gt;To explain what a project is about, talk is cheap, and a video is worth more than a thousand words. If you want to know what the Dove app actually looks like, here is a video where I demo the platform.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/c8n53Zhl4TM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><category term="blog" /><category term="hackathon" /><summary type="html">In December 2019, I participated in a hackathon organized in Ashesi: the HackAPI. This article will explain the solution my team built.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jean72human.github.io/ml-blog/images/project-dove/dove-screen.PNG" /><media:content medium="image" url="https://jean72human.github.io/ml-blog/images/project-dove/dove-screen.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">My first real team experience in a Zindi competition</title><link href="https://jean72human.github.io/ml-blog/blog/competitions/zindi/2020/09/20/my-first-real-team-experience-in-a-zindi-competition.html" rel="alternate" type="text/html" title="My first real team experience in a Zindi competition" /><published>2020-09-20T00:00:00-05:00</published><updated>2020-09-20T00:00:00-05:00</updated><id>https://jean72human.github.io/ml-blog/blog/competitions/zindi/2020/09/20/my-first-real-team-experience-in-a-zindi-competition</id><content type="html" xml:base="https://jean72human.github.io/ml-blog/blog/competitions/zindi/2020/09/20/my-first-real-team-experience-in-a-zindi-competition.html">&lt;p&gt;I love competitions, especially in data science. After getting started with machine learning, competing on Kaggle and Zindi became one of my favourite hobbies. But between the lack of time, computing resource and experience, I ended my first challenges closer to the bottom of the private leaderboard. Finally, I decided to listen to one excellent advice often given to people starting on Kaggle. I think it was something like this:
“ To win, you need good ensembles; good ensembles are diverse. To get diverse ensembles, you need a good team.”
Based on this advice, I became more open to working in a team. I did, for a Zindi competition and it worked: me and my friend finished &lt;a href=&quot;https://zindi.africa/competitions/basic-needs-basic-rights-kenya-tech4mentalhealth/leaderboard&quot;&gt;7th&lt;/a&gt; which is better than I did in any previous competition. No cash (yet) but a lot of learning.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The team&lt;/h2&gt;

&lt;p&gt;My first real team experience was in a team of two, with Ronny Polle. If you don’t know him, he’s a medical student who likes researching challenging machine learning problems. I met him at the Deep Learning Indaba 2019 in Nairobi. For the &lt;a href=&quot;https://zindi.africa/competitions/basic-needs-basic-rights-kenya-tech4mentalhealth&quot;&gt;Basic Needs Basic Rights - Tech4MentalHealth competition&lt;/a&gt;, we formed a team of two. The task consisted of classifying statements from Kenyan university students in terms of the mental health challenges they struggle with.&lt;/p&gt;

&lt;p&gt;Overall the experience was quite pleasing and very different from working alone. The aspect I liked the most, apart from the obvious boost on the leaderboard, was the process when coming up with new ideas. A big part of scoring well on such competitions is coming up with new ideas fast and experimenting them. Unfortunately, generating new ideas after seeing the past twenty fail is not always an easy thing to do. When you are not alone, the burden of generating ideas is shared. Even better, your ideas bounce off each other to form even more ideas. So many that it eventually becomes hard to evaluate them all, which was not facilitated by the platform. On Zindi, the daily submission limitations are the same for both teams and individuals, meaning that each individual in a team can make fewer submissions than an individual working solo. These limitations can become frustrating before you even realize it. Despite the pains of having rigid restrictions in terms of submissions, the overall experience was a positive one.&lt;/p&gt;

&lt;h2 id=&quot;the-learning-experience&quot;&gt;The learning experience&lt;/h2&gt;

&lt;h3 id=&quot;what-i-learned-concerning-machine-learning&quot;&gt;What I learned concerning machine learning&lt;/h3&gt;
&lt;p&gt;In terms of machine learning, the main thing I learned during this competition is how important cross-validation is. In general, accurately evaluating your models is primordial. Without a proper way of determining the performance of your model, you won’t be able to compare them, and know what is working and what is not. 
Apart from that, the challenge helped me confirm a few things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;pretraining is important&lt;/li&gt;
  &lt;li&gt;when the dataset is small, bigger models do not always help&lt;/li&gt;
  &lt;li&gt;most of the performance boost comes from tuning hyperparameters
The few points mentioned above can seem evident for some, but the quality of the execution of those simple things is what makes a real difference.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-i-learned-concerning-data-science-competitions&quot;&gt;What I learned concerning data science competitions&lt;/h3&gt;
&lt;p&gt;Relative to competitions, what I learned comes down to this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Seed every seedable”: Zindi will drop you mercilessly if your code does not reproduce the results of your best CSV file, and rightly so. To avoid reproducibility issues, set a fixed value for every random seed that comes to mind. I wish we knew that earlier as the score of our best CSV file is better than the final best score on the private leaderboard.&lt;/li&gt;
  &lt;li&gt;Keep every notebook: you can never really tell which notebook or code will give you your best score, so keep every single code file.&lt;/li&gt;
  &lt;li&gt;Experiment ideas in quantity and quality: any method cannot truly be ruled out until we see it fail on the leaderboard, even then, big &lt;a href=&quot;https://www.theclickreader.com/how-we-lost-30000-on-a-kaggle-competition-2020/&quot;&gt;shake-ups&lt;/a&gt; are a thing. Trying as many ideas as possible, then testing them with cross-validation and the leaderboard is the way to go.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-results&quot;&gt;The results&lt;/h2&gt;

&lt;p&gt;In the end, I was quite satisfied with the results. Teaming up helped me get my first top 10 spot in a data science competition by finishing 7th. I also learned a lot with regards to both machine learning and competitive data science in particular. 
I didn’t talk a lot about the final submission, because it was quite basic and was not anything innovative or interesting. The model used was a pre-trained RoBertA model with training and predictions made using 8-fold cross-validation. You can find it on GitHub &lt;a href=&quot;https://github.com/DrCod/Zindi-Tech4MentalHealth-NLP-Challenge&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="competitions" /><category term="zindi" /><summary type="html">I love competitions, especially in data science. After getting started with machine learning, competing on Kaggle and Zindi became one of my favourite hobbies. But between the lack of time, computing resource and experience, I ended my first challenges closer to the bottom of the private leaderboard. Finally, I decided to listen to one excellent advice often given to people starting on Kaggle. I think it was something like this: “ To win, you need good ensembles; good ensembles are diverse. To get diverse ensembles, you need a good team.” Based on this advice, I became more open to working in a team. I did, for a Zindi competition and it worked: me and my friend finished 7th which is better than I did in any previous competition. No cash (yet) but a lot of learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jean72human.github.io/ml-blog/images/zindi-nlpx/nlpx.PNG" /><media:content medium="image" url="https://jean72human.github.io/ml-blog/images/zindi-nlpx/nlpx.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Learning for Video Classification</title><link href="https://jean72human.github.io/ml-blog/blog/2020/08/17/deep-learning-for-video-classification.html" rel="alternate" type="text/html" title="Deep Learning for Video Classification" /><published>2020-08-17T00:00:00-05:00</published><updated>2020-08-17T00:00:00-05:00</updated><id>https://jean72human.github.io/ml-blog/blog/2020/08/17/deep-learning-for-video-classification</id><content type="html" xml:base="https://jean72human.github.io/ml-blog/blog/2020/08/17/deep-learning-for-video-classification.html">&lt;p&gt;In my third year of BSc at Ashesi University ( 2019 spring semester), I took a machine learning course. That course was one of my first introduction to the mathematics behind machine learning concepts such as Naive Bayes and deep neural networks. At the end of the semester, in teams of five, we worked on projects that would require us to push ourselves a bit and implement a model that cannot just be imported from a library. My team decided to work on video classification, more specifically, we worked on automatic lip-reading using video inputs. Fast forward to a few weeks ago, the AI Ghana community contacted me. I was asked to present on a machine learning topic of my choice, and I thought it would be an excellent idea to give a talk on video classification using deep learning.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;p&gt;They recorded the presentation and uploaded it to Youtube so here it is:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/x2RG0lpzo1c&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;github&quot;&gt;Github&lt;/h2&gt;

&lt;p&gt;I also uploaded the code on my Github &lt;a href=&quot;https://github.com/jean72human/deep-learning-for-video-classification&quot;&gt;here&lt;/a&gt;. It implemented it using TensorFlow Keras. Note that the dataset used here doesn’t make any learning possible so do not expect anything when looking at the loss graphs. This code is just here to illustrate how a deep learning model for video classification would look like and how to train it.&lt;/p&gt;</content><author><name></name></author><category term="blog" /><summary type="html">In my third year of BSc at Ashesi University ( 2019 spring semester), I took a machine learning course. That course was one of my first introduction to the mathematics behind machine learning concepts such as Naive Bayes and deep neural networks. At the end of the semester, in teams of five, we worked on projects that would require us to push ourselves a bit and implement a model that cannot just be imported from a library. My team decided to work on video classification, more specifically, we worked on automatic lip-reading using video inputs. Fast forward to a few weeks ago, the AI Ghana community contacted me. I was asked to present on a machine learning topic of my choice, and I thought it would be an excellent idea to give a talk on video classification using deep learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jean72human.github.io/ml-blog/images/dl-for-vid-cl/dl-for-vid-cl.PNG" /><media:content medium="image" url="https://jean72human.github.io/ml-blog/images/dl-for-vid-cl/dl-for-vid-cl.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi-source transfer learning and web scraping</title><link href="https://jean72human.github.io/ml-blog/research/2020/03/20/multi-source-transfer-learning-and-web-scraping.html" rel="alternate" type="text/html" title="Multi-source transfer learning and web scraping" /><published>2020-03-20T00:00:00-05:00</published><updated>2020-03-20T00:00:00-05:00</updated><id>https://jean72human.github.io/ml-blog/research/2020/03/20/multi-source-transfer-learning-and-web-scraping</id><content type="html" xml:base="https://jean72human.github.io/ml-blog/research/2020/03/20/multi-source-transfer-learning-and-web-scraping.html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Machine learning and, more specifically, deep learning have recently driven many innovations. The availability of massive datasets and computation resources has made it possible to create deeper neural networks that are able to learn more meaningful representations of the data. Those new possibilities are not always accessible to the average African company trying to leverage on deep learning to increase profit. In that case, scarcity of data, especially, could be a limitation since neural networks are known to be data-hungry. When faced with the issue of unavailability of public data, a company can either increase the size of the dataset by collecting data themselves or increase the size and complexity of the model. The option studied here is to use web scraping to manage and clean a bigger dataset. In trying to increase the size and complexity of the model, to avoid overfitting, the transfer learning approach was used. This technique involves the transfer of weights from several datasets using model ensembling. All these methods were tested on a rice meal classification problem. The problem consists of classifying images of four rice-based dishes: jollof rice, fried rice, plain rice, and waakye. The dataset contains 60 train images and 20 test images for each group making up a total of 240 training images and 80 testing images. The baseline of 75% was achieved using a dense net Convolutional Neural Network (CNN). The web scraping method used to increase the dataset size attained an accuracy of 87%. A multi-source transfer learning approach was also used where models were pre-trained on the Food-101 dataset and the Food-256 dataset. The multisource transfer learning method achieved an accuracy of 90%. Using these two methods, we implement two ways to significantly increase the efficiency of a model when the original dataset is small.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The size of training data plays a vital role in Machine Learning (ML), in learning useful representations that accurately predict the task at hand. Generally, it is common knowledge that a small dataset will result in a sparse approximation of the underlying regularity, resulting in a model with abysmal performance. Techniques have been proposed in ML literature to facilitate selecting the best model in the face of a small training dataset. Methods include employing a short model with fewer parameters, cross-validation &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, transfer learning &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, among others. However, too little data size does not often yield much improvement with these approaches. Most importantly, in the African context where data is often highly unstructured and available in minimal quantities, building models with high accuracy calls for a different approach.  &lt;br /&gt;
This paper, demonstrates how to achieve models with high predictive accuracy, when faced with data that is highly unstructured or unavailable, using Multi-Source Transfer learning (Model Ensembling) or Web Scraping to assemble a structured dataset set for most ML tasks. Multi-Source Transfer is a common practice employed by participants in ML competitions to build winning models, while Web Scraping can be applied to create datasets in a context where no data infrastructure exists to facilitate dataset creation.&lt;/p&gt;

&lt;h2 id=&quot;materials-and-methods&quot;&gt;Materials and methods&lt;/h2&gt;

&lt;h3 id=&quot;materials&quot;&gt;Materials&lt;/h3&gt;

&lt;p&gt;The following are some of the technologies we used in the project.  &lt;br /&gt;
PyTorch: Pytorch, is a deep learning package, which is known for its high-level tensor computations and building neural networks with less effort. Pytorch is Pythonic, and more importantly, highly optimized for computationally expensive operations, such as convolutional neural networks, recurrence neural networks, and complex tensor operations. Pytorch has many pre-trained models that enhance speedy model training with appreciable high accuracy. This package is still a young player compared to its competitors like TensorFlow. However, it gains momentum very fast due to its features above.  &lt;br /&gt;
Floyd Hub: Training deep learning models is computationally expensive; it requires machines with high processing power. The cheapest way to train the models is to purchase cloud computing services if buying a GPU for your local device is expensive. Floydhub is a cloud computing option employed to train the model under study, because they already preinstalled TensorFlow, PyTorch, Keras, and many more dependencies. Quite apart from having an extensive collection of pre-installed dependencies, Floydhub is simple to use.  &lt;br /&gt;
Google-images-download: This technology is a python package utility for conveniently scraping images from google. However, other powerful web scraping tools exist.&lt;/p&gt;

&lt;h3 id=&quot;multi-source-transfer-learning&quot;&gt;Multi-source transfer learning&lt;/h3&gt;

&lt;p&gt;Transfer learning consists of transferring knowledge from a more extensive database (source) to a smaller database (target) using weights learned from the bigger database as a starting point when training a model for the second database. The domain of the source and how closely it is related to the domain of the target is also relevant since it can increase the efficiency of the transfer.  In this experiment, weights are transfered from several sources, which is known as multi-source transfer learning. One limitation of multi-source transfer learning is that multiple sets of weights cannot be used as a starting point. Hence the use of a model ensembling method to transfer knowledge from all the datasets &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.  &lt;br /&gt;
Multi-Source Transfer or Model Ensembling consists of pooling together the predictions of a set of different models to produce better forecasts – where the final model is trained for each of the sources. To fuse the knowledge from these various models, we used an ensemble that concatenates features extracted using the models trained on the sources, and passed the concatenated features to a multilayer perceptron (MLP). The ensemble was then fine-tuned on the target. The models were pre-trained on four datasets: the original dataset, ImageNet, Food101 &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, and Food-256 [^5].&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml-blog/images/transferlearningone/diagramone.jpg&quot; alt=&quot;&quot; title=&quot;Figure 1: Diagram of the model ensemble used for multi-source transfer learning&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;web-scraping&quot;&gt;Web scraping&lt;/h3&gt;

&lt;p&gt;Web Scraping or web data extraction is used for extracting human-readable data from websites. It is a handy technique used in scenarios where data for a specific ML task is complicated to come by. There are not a lot of data infrastructure that exists in most organizations in the African context.  Furthermore, due to bureaucracy and the lack of trust of organizations exposing their data, this technique is powerful for collecting data for various ML tasks in the Africa context. 
Using this method, two thousand training images of local rice dishes were scrapped in less than thirty (30) minutes. Since a lot of junk images are often downloaded during web scraping, some manual work was done to remove unwanted files and clean up the data into a suitable format.&lt;/p&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;The methods tested were implemented using PyTorch and run on a virtual machine with a Tesla V100 GPU (16 GB ram) and 8 CPUs. All models were trained with a learning rate of 1e-3. The pre-trained models for Food-101 and Food-256 were trained for five epochs. The models (baselines and ensemble) were trained for 30 epochs.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;As a baseline method, a randomly initialized densenet-121 model on the initial dataset was trained. Using this method, an accuracy of 87% was achieved. Table 1 provides a summary of the results obtained.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline (randomly initialized densenet-121)&lt;/td&gt;
      &lt;td&gt;75%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Increased dataset&lt;/td&gt;
      &lt;td&gt;87%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Multi-source transfer learning&lt;/td&gt;
      &lt;td&gt;90%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The pre-trained ensemble model achieved the highest accuracy and had a consistently high accuracy across epochs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ml-blog/images/transferlearningone/graphone.jpg&quot; alt=&quot;&quot; title=&quot;Figure 2: Accuracies for models N0 to N4&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;After increasing the size of the dataset, with the same method, there was a substantial increase in the level of accuracy. This improvement shows the importance of large data sets, and the limitations of deep learning techniques when dealing with small data. Therefore, for deep learning to be used to its full potential in Africa, it is important that quality local datasets are made public, in order to foster research and make it easier for companies to profit from the deep learning advancement. It also showcases how useful web scraping can be as a data science tool for dataset creation. &lt;br /&gt;
Multi-source transfer learning resulted in a model that was able to reach high accuracies quite fast. The proposed ensemble also seems not to overfit and shows a stable increase in accuracy. Sharing pre-trained models can greatly help when faced with limited data, especially with pretrained models on various datasets. Our approach could have been made easier to implement if all the pre-trained models were already available. Also, so far this approach would not be suited for all types of inferences because of the big size of the models and their high latency. This can, however, be solved using model distillation and model compression.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, this study showed has how easy it is to obtain a significant improvement in accuracy (up to 15%) over the baseline results, using Multi-Source Transfer Learning. The study also showed how to achieve a significant improvement in accuracy (up to 12%) over the baseline results by employing Web Scraping technique to acquire more dataset for a specific task. There is still a great avenue to improve upon these accuracies. We estimate that we can achieve a very high overall accuracy when these two techniques are combined. In our future work, we hope to test this hypothesis by combining these two techniques.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;R. Kohavi, “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection”, Research gate, 2001. Available: https://www.researchgate.net/profile/Ron_Kohavi/publication/2352264_A_Study_of_CrossValidation_and_Bootstrap_for_Accuracy_Estimation_and_Model_Selection/links/02e7e51bc c14c5e91c000000.pdf. [Accessed 12 June 2019]. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ng.  Hong-Wei &amp;amp; Nguyen, D. Vonikakis, V. Winkler, Stefan. “ Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning”. Available: 10.1145/2818346.2830593. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe and S. Mougiakakou, “Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis”, IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 76-84, 2017. Available: 10.1109/jbhi.2016.2636929. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;L. Bossard, M. Guillaumin and L. Van Gool, “Food-101 – Mining Discriminative Components with Random Forests”, Vision.ee.ethz.ch, 2019. [Online]. Available: https://www.vision.ee.ethz.ch/datasets_extra/food-101/. [Accessed: 13- Jun- 2019] &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="research" /><summary type="html">Abstract</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jean72human.github.io/ml-blog/images/transferlearningone/diagramone.jpg" /><media:content medium="image" url="https://jean72human.github.io/ml-blog/images/transferlearningone/diagramone.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>