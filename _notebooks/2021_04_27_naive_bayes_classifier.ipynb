{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-04-27-naive-bayes-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boOHW-gVUlsh"
      },
      "source": [
        "# AMMI notes - Naive Bayes classifier for sentiment analysis\n",
        "> Using a Naive Bayes classifier to classify reviews\n",
        "\n",
        "- toc:true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Gbetondji Dovonon\n",
        "- categories: [notes,ammi,blog]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-zHABm0nMT"
      },
      "source": [
        "This is a simple implementation of Naive Bayes classifier for review rating. The classifier uses Baye's rule to determine the probability of a text belonging to a certain class, then we find the class with the highest probability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEkvf4z2phT"
      },
      "source": [
        "Let's not a token belonging to a sentence as $w_i$ where $i$ is its position in the sentence. We can find the probability $P(c | w_1 w_2 ... w_n)$ for a piece of text with $n$ tokens. Using Baye's rule, and assuming independence of the tokens with respect to each other, we have\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P(c | w_1 w_2 ... w_n) & \\propto P(c) P( w_1 w_2 ... w_n | c) \\\\\n",
        "& \\propto P(c) P( w_1 | c) P (w_2 | c) ... P( w_n | c) \\\\\n",
        "& \\propto P(c) \\prod_{i=0}^{n} P( w_i | c) \\\\\n",
        "& \\propto \\log P(c) + \\sum_{i=0}^{n} \\log P( w_i | c) \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This model doesn't directly use the probability $P(c | w_1 w_2 ... w_n)$ but rather uses the likelihood $P( w_1 w_2 ... w_n | c)$ and prior $P(c)$. It therefore belongs to the family of generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkxOohAfW9xl"
      },
      "source": [
        "We can use counts to approximate those probabilties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4Auo_zXGi2"
      },
      "source": [
        "$$\n",
        "P(w_i | c) = \\frac{count(w_i)}{count(c)}\n",
        "$$\n",
        "with $count(w_i)$ being the number of times the token $w_i$ appears in class $c$ and $count(c)$ the number of examples in that class.\n",
        "$$\n",
        "P(c) = \\frac{count(c)}{|\\mathcal{D}|}\n",
        "$$\n",
        "with $count(c)$ the number of examples in that class and $|\\mathcal{D}|$ the size of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfJvxB9k7SP3"
      },
      "source": [
        "We will try the classifier on review data. We will combine labeled review data from Amazon, Imdb and Yelp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISQcIZHIXth5"
      },
      "source": [
        "!wget -O amazon_cells_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/amazon_cells_labelled.txt\n",
        "!wget -O imdb_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/imdb_labelled.txt\n",
        "!wget -O yelp_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/yelp_labelled.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAhqJZZr75oS"
      },
      "source": [
        "We will use the nltk python package for preprocessing. More specifically, it will be the punkt tokenizer and wordnet lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR6utSW9YNXg"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVKuHcYV3Uh"
      },
      "source": [
        "Now we have the code for the class. The notable methods here are: \n",
        "- `_train`: it collects the various statistics we need for our classifier\n",
        "- `_predict`: uses those statisctics to make predictions\n",
        "- `preprocess`: takes in the sentence and does the tokenization, normalization and lemmatization\n",
        "\n",
        "I also added a method to load and save the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4jT6hHXWwck"
      },
      "source": [
        "### NAIVE BAYES CLASS\n",
        "\n",
        "\n",
        "class naive_classifier:\n",
        "    def __init__(self, classes):\n",
        "        \"\"\"\n",
        "        Initialization of the \n",
        "        \"\"\"\n",
        "        self.trained = False\n",
        "        self.classes = classes\n",
        "        self.nclasses = len(self.classes)\n",
        "        \n",
        "        self.likelihoods = {c : dict() for c in range(self.nclasses) }\n",
        "        self.priors = [0 for i in range(self.nclasses)]\n",
        "        self.vocabulary = []\n",
        "\n",
        "    def _train(self, corpus):\n",
        "        classCounts = [0 for i in range(self.nclasses)]\n",
        "        ndoc = len(corpus)\n",
        "        wordCounts = {c : dict() for c in range(self.nclasses)}\n",
        "        for document in corpus:\n",
        "            review = document[0]\n",
        "            label = document[-1]\n",
        "            classCounts[label] += 1\n",
        "            for word in review:\n",
        "                if word in wordCounts[label].keys():\n",
        "                    wordCounts[label][word] += 1\n",
        "                else:\n",
        "                    wordCounts[label][word] = 1\n",
        "                    \n",
        "        for index in range(len(self.classes)):\n",
        "            self.priors[index] = np.log(classCounts[index]/ndoc)\n",
        "            self.vocabulary += list(wordCounts[index].keys())\n",
        "        self.vocabulary = set(self.vocabulary)\n",
        "        print (\"Vocabulary size: \",len(self.vocabulary))\n",
        "            \n",
        "        for index in range(len(self.classes)):\n",
        "            for word in self.vocabulary:\n",
        "                if word in wordCounts[index]:\n",
        "                    self.likelihoods[index][word] = np.log((wordCounts[index][word]+1)/(sum(wordCounts[index].values())+len(wordCounts[index])))\n",
        "                else:\n",
        "                    self.likelihoods[index][word] = np.log((1)/(sum(wordCounts[index].values())+len(wordCounts[index])))            \n",
        "        \n",
        "    def _read(self, document):\n",
        "        toReturn = []\n",
        "        with open(document) as f:\n",
        "            for line in f.readlines():\n",
        "                pair = line.split('\\n')\n",
        "                pair = pair[0].split('\\t')\n",
        "                review = re.sub(r\"[,?!-()*&^%|'.,]\",\"\",pair[0]) ## normalization\n",
        "                bag = self.preprocess(pair[0].lower())\n",
        "                label = int(pair[1])\n",
        "                toReturn.append((bag,label))\n",
        "        return toReturn\n",
        "    \n",
        "    def preprocess(self, sentence):\n",
        "        \"\"\"\n",
        "        preprocesses the sentences. Tokenizes them, then lemmatizes the token using the wordnet lemmatizer\n",
        "        \"\"\"\n",
        "        import string\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        words = word_tokenize(sentence)\n",
        "        toReturn = []\n",
        "        for word in words:\n",
        "            if (word not in string.punctuation):\n",
        "                toReturn.append(wordnet_lemmatizer.lemmatize(word))\n",
        "        return toReturn\n",
        "    \n",
        "    def train(self, documents, test=False, split_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Takes txt inputs and trains the classifier\n",
        "        \"\"\"\n",
        "        corpus = []\n",
        "        for doc in documents:\n",
        "            print (\"reading: \",doc)\n",
        "            for review in self._read(doc):\n",
        "                corpus.append(review)\n",
        "                \n",
        "        if test:\n",
        "            np.random.shuffle(corpus)\n",
        "            split_point = int(len(corpus) * split_ratio)\n",
        "            test_data = corpus[:split_point]\n",
        "            train_data = corpus[split_point:]\n",
        "            self._train(train_data)\n",
        "            test_acc = self._test(test_data)\n",
        "            train_acc = self._test(train_data)\n",
        "            print (len(train_data),\" training items\")\n",
        "            print (len(test_data),\" testing items\")\n",
        "            print (\"Training done\")\n",
        "            print (\"Train accuracy: \",train_acc)\n",
        "            print (\"Test accuracy: \",test_acc)\n",
        "        else:\n",
        "            self._train(corpus)\n",
        "            print (\"Training done\")\n",
        "        self.trained = True\n",
        "        \n",
        "    def _predict(self, sentence):\n",
        "        \"\"\"\n",
        "        Takes tokenized input and outputs numerical class\n",
        "        \"\"\"\n",
        "        import operator\n",
        "        sumc = dict()\n",
        "        for c in range(self.nclasses):\n",
        "            sumc[c] = self.priors[c]\n",
        "            for word in sentence:\n",
        "                if word in self.vocabulary:\n",
        "                    sumc[c] += self.likelihoods[c][word]\n",
        "        return max(sumc.items(), key=operator.itemgetter(1))[0]\n",
        "    \n",
        "    def predict(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize sentence, predicts and output class\n",
        "        \"\"\"\n",
        "        sentence = self.preprocess(text)\n",
        "        return self._predict(sentence)\n",
        "    \n",
        "    def _test(self, data):\n",
        "        n_items = len(data)\n",
        "        n_correct = 0\n",
        "        for document in data:\n",
        "            review = document[0]\n",
        "            label = document[-1]\n",
        "            c = self._predict(review)\n",
        "            if (c == label): n_correct += 1\n",
        "        return n_correct / n_items  \n",
        "    \n",
        "    def export(self, name):\n",
        "        import json\n",
        "        \n",
        "        toExport = {\n",
        "            \"likelihoods\":self.likelihoods,\n",
        "            \"priors\":self.priors,\n",
        "            \"vocabulary\":self.vocabulary\n",
        "        }\n",
        "        \n",
        "        np.save(name, toExport)\n",
        "            \n",
        "    def load(self, name):\n",
        "        import json\n",
        "        \n",
        "        loaded = np.load(name)\n",
        "            \n",
        "        self.likelihoods = loaded.item().get(\"likelihoods\")\n",
        "        self.priors = loaded.item().get(\"priors\")\n",
        "        self.vocabulary = loaded.item().get(\"vocabulary\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DacVtxlL8qLi"
      },
      "source": [
        "We train and test the classifier. We can see that even a naive classifier can achieve a relatively good accuracy on simple problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfWLsVhbXkDH",
        "outputId": "9c76ed28-b317-4b11-da64-57ffc9dc2681"
      },
      "source": [
        "classifier = naive_classifier(classes = [\"positive\", \"negative\"])\n",
        "classifier.train([\"./amazon_cells_labelled.txt\",\n",
        "                  \"./imdb_labelled.txt\",\n",
        "                  \"./yelp_labelled.txt\"],\n",
        "                 test=True,\n",
        "                 split_ratio=0.2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading:  ./amazon_cells_labelled.txt\n",
            "reading:  ./imdb_labelled.txt\n",
            "reading:  ./yelp_labelled.txt\n",
            "Vocabulary size:  4371\n",
            "2400  training items\n",
            "600  testing items\n",
            "Training done\n",
            "Train accuracy:  0.93875\n",
            "Test accuracy:  0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZHuwcuVqSD"
      },
      "source": [
        "We can now test the classifier with any data at all and see the result. Here is an example that belongs to the positive class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Tq4vDUb5Fn",
        "outputId": "e87bf73f-7f67-4767-c281-2294b1f20f9a"
      },
      "source": [
        "test_text1 = \"Mushoku Tensei is the greatest light novel series ever! It has great world building, foreshadowing and character development\"\n",
        "\n",
        "classifier.predict(test_text1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3X0Si9eVyVG"
      },
      "source": [
        "Here is one that belongs to the negative class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvLENFs1oVF",
        "outputId": "4127bc88-5e9c-4b03-f293-52d194ac9076"
      },
      "source": [
        "test_text2 = \"The new iPhone was not satisfactory. The innovation is not there anymore and prices are still through the roof\"\n",
        "\n",
        "classifier.predict(test_text2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}