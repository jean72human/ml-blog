{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boOHW-gVUlsh"
   },
   "source": [
    "# AMMI notes - Naive Bayes classifier for sentiment analysis\n",
    "> Using a Naive Bayes classifier to classify reviews\n",
    "\n",
    "- toc:true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Gbetondji Dovonon\n",
    "- categories: [notes,ammi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iE-zHABm0nMT"
   },
   "source": [
    "This is a simple implementation of Naive Bayes classifier for review rating. The classifier uses Bayes' rule to determine the probability of a text belonging to a certain class, then we find the class with the highest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITEkvf4z2phT"
   },
   "source": [
    "Let a token belonging to a sentence be $w_i$ where $i$ is its position in the sentence. We can find the probability $P(c | w_1 w_2 ... w_n)$ for a piece of text with $n$ tokens. Using Baye's rule (we can ignore the evidence term since it doesn't change no matter the class), and assuming independence of the tokens with respect to each other, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(c | w_1 w_2 ... w_n) & \\propto P(c) P( w_1 w_2 ... w_n | c) \\\\\n",
    "& \\propto P(c) P( w_1 | c) P (w_2 | c) ... P( w_n | c) \\\\\n",
    "& \\propto P(c) \\prod_{i=0}^{n} P( w_i | c) \\\\\n",
    "& \\propto \\log P(c) + \\sum_{i=0}^{n} \\log P( w_i | c) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This model doesn't directly use the probability $P(c | w_1 w_2 ... w_n)$ but rather uses the likelihood $P( w_1 w_2 ... w_n | c)$ and prior $P(c)$. It therefore belongs to the family of generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkxOohAfW9xl"
   },
   "source": [
    "We can use counts to approximate those probabilties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4Auo_zXGi2"
   },
   "source": [
    "$$\n",
    "P(w_i | c) = \\frac{count(w_i)}{count(c)}\n",
    "$$\n",
    "with $count(w_i)$ being the number of times the token $w_i$ appears in class $c$ and $count(c)$ the number of examples in that class.\n",
    "$$\n",
    "P(c) = \\frac{count(c)}{|\\mathcal{D}|}\n",
    "$$\n",
    "with $count(c)$ the number of examples in that class and $|\\mathcal{D}|$ the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfJvxB9k7SP3"
   },
   "source": [
    "We will try the classifier on review data. We will combine labeled review data from Amazon, Imdb and Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISQcIZHIXth5"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "!wget -O amazon_cells_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/amazon_cells_labelled.txt\n",
    "!wget -O imdb_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/imdb_labelled.txt\n",
    "!wget -O yelp_labelled.txt https://raw.githubusercontent.com/jean72human/nlp-2018-jean72human/master/project1/sentiment_labelled_sentences/yelp_labelled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhqJZZr75oS"
   },
   "source": [
    "We will use the nltk python package for preprocessing. More specifically, it will be the punkt tokenizer and wordnet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qR6utSW9YNXg"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDVKuHcYV3Uh"
   },
   "source": [
    "Now we have the code for the class. The notable methods here are: \n",
    "- `_train`: it collects the various statistics we need for our classifier\n",
    "- `_predict`: uses those statisctics to make predictions\n",
    "- `preprocess`: takes in the sentence and does the tokenization, normalization and lemmatization\n",
    "\n",
    "I also added a method to load and save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the initializer for our naive classifier\n",
    "```python\n",
    "class naive_classifier:\n",
    "    def __init__(self, classes):\n",
    "            \"\"\"\n",
    "            Initialization of the object. We create the dictionaries that will hold the priors and likelihoods.\n",
    "            \"\"\"\n",
    "            self.trained = False\n",
    "            self.classes = classes\n",
    "            self.nclasses = len(self.classes)\n",
    "\n",
    "            self.likelihoods = {c : dict() for c in range(self.nclasses) }\n",
    "            self.priors = [0 for i in range(self.nclasses)]\n",
    "            self.vocabulary = []\n",
    "```\n",
    "It only takes `classes` as argument. That would be a list of classes to predict like `[\"postive\",\"negative]` for instance. \n",
    "There is a `trained` boolean that will be set to true once training is done. `n_classes` is the number of classes. The `likelihoods` variable will be a dictionary that will contain as many dictionaries as there are classes. The dictionaries inside that pariable will contain the probabilities of the words belonging to that class, $P( w_i | c) $ in the equations above. It would look somehow like this:\n",
    "```python\n",
    "{\n",
    "    \"positive\": {\n",
    "        \"bad\": 0.007, # probability of the word 'bad' being in a postive review\n",
    "        \"great\": 0.401, # probability of the word 'great' being in a postive review\n",
    "        ...\n",
    "        \"good\": 0.576, # probability of the word 'good' being in a postive review\n",
    "    },\n",
    "    \"negative\": {\n",
    "        \"bad\": 0.72, # probability of the word 'bad' being in a negative review\n",
    "        \"great\": 0.0001, # probability of the word 'great' being in a negative review\n",
    "        ...\n",
    "        \"good\": 0.016, # probability of the word 'good' being in a negative review\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "There's a `prior` for the probabilities of each class, $P(c)$ in the equations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will skip the read method. You can check it out by running the notebook. Just go to the top of the page and click on \"open in colab\". You can also use \"view on github\".\n",
    "\n",
    "The first method we are interested in is the preprocessing method.\n",
    "```python\n",
    "def preprocess(self, sentence):\n",
    "    \"\"\"\n",
    "    preprocesses the sentences. Tokenizes them, then lemmatizes the token using the wordnet lemmatizer\n",
    "    \"\"\"\n",
    "    import string\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(sentence)\n",
    "    toReturn = []\n",
    "    for word in words:\n",
    "        if (word not in string.punctuation):\n",
    "            toReturn.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return toReturn\n",
    "```\n",
    "It takes in a string object. NLTK provides us with a tokenizer, it will break the sentence into a list of tokens. We will then replace each token by its lemma using the wordnet lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training function:\n",
    "```python\n",
    "def _train(self, corpus):\n",
    "    classCounts = [0 for i in range(self.nclasses)]\n",
    "    ndoc = len(corpus)\n",
    "    wordCounts = {c : dict() for c in range(self.nclasses)}\n",
    "    for document in corpus:\n",
    "        review = document[0]\n",
    "        label = document[-1]\n",
    "        classCounts[label] += 1\n",
    "        for word in review:\n",
    "            if word in wordCounts[label].keys():\n",
    "                wordCounts[label][word] += 1\n",
    "            else:\n",
    "                wordCounts[label][word] = 1\n",
    "\n",
    "    for index in range(len(self.classes)):\n",
    "        self.priors[index] = np.log(classCounts[index]/ndoc)\n",
    "        self.vocabulary += list(wordCounts[index].keys())\n",
    "    self.vocabulary = set(self.vocabulary)\n",
    "    print (\"Vocabulary size: \",len(self.vocabulary))\n",
    "\n",
    "    for index in range(len(self.classes)):\n",
    "        for word in self.vocabulary:\n",
    "            if word in wordCounts[index]:\n",
    "                numerator = wordCounts[index][word] + 1 \n",
    "                denominator = sum(wordCounts[index].values()) + len(wordCounts[index])\n",
    "                self.likelihoods[index][word] = np.log(numerator/denominator)\n",
    "            else:\n",
    "                ## for words that are not yet in the vocabulary, we start from one\n",
    "                numerator = 1\n",
    "                denominator = sum(wordCounts[index].values())+len(wordCounts[index])\n",
    "                self.likelihoods[index][word] = np.log(numerator/denominator)          \n",
    "\n",
    "```\n",
    "The first for loop of this code collects the counts in a dictionary called `wordCounts`. The second one computes the priors for each class and creates the vocabulary. The vocabulary is the combined set of tokens from all the classes.  The last loop then uses the compiled statistics to compute the likelihoods for each word within each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the predict function.\n",
    "\n",
    "```python\n",
    "def _predict(self, sentence):\n",
    "    \"\"\"\n",
    "    Takes tokenized input and outputs numerical class\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    sumc = dict()\n",
    "    for c in range(self.nclasses):\n",
    "        sumc[c] = self.priors[c]\n",
    "        for word in sentence:\n",
    "            if word in self.vocabulary:\n",
    "                sumc[c] += self.likelihoods[c][word]\n",
    "    return max(sumc.items(), key=operator.itemgetter(1))[0]\n",
    "```\n",
    "\n",
    "In this method, we loop trough the classes and compute the value of $\\log P(c) + \\sum_{i=0}^{n} \\log P( w_i | c)$. We put them in a variable called sumc which is a dictionary with the classes as keys. We first add the class prior, then we add the likelihoods of each token for that specific class. \n",
    "THe last line then gives us the key with the maximum value, that is the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "A4jT6hHXWwck"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "### NAIVE BAYES CLASS\n",
    "\n",
    "\n",
    "class naive_classifier:\n",
    "    def __init__(self, classes):\n",
    "        \"\"\"\n",
    "        Initialization of the object. We create the dictionaries that will hold the priors and likelihoods.\n",
    "        \"\"\"\n",
    "        self.trained = False\n",
    "        self.classes = classes\n",
    "        self.nclasses = len(self.classes)\n",
    "        \n",
    "        self.likelihoods = {c : dict() for c in range(self.nclasses) }\n",
    "        self.priors = [0 for i in range(self.nclasses)]\n",
    "        self.vocabulary = []\n",
    "\n",
    "    def _train(self, corpus):\n",
    "        classCounts = [0 for i in range(self.nclasses)]\n",
    "        ndoc = len(corpus)\n",
    "        wordCounts = {c : dict() for c in range(self.nclasses)}\n",
    "        for document in corpus:\n",
    "            review = document[0]\n",
    "            label = document[-1]\n",
    "            classCounts[label] += 1\n",
    "            for word in review:\n",
    "                if word in wordCounts[label].keys():\n",
    "                    wordCounts[label][word] += 1\n",
    "                else:\n",
    "                    wordCounts[label][word] = 1\n",
    "                    \n",
    "        for index in range(len(self.classes)):\n",
    "            self.priors[index] = np.log(classCounts[index]/ndoc)\n",
    "            self.vocabulary += list(wordCounts[index].keys())\n",
    "        self.vocabulary = set(self.vocabulary)\n",
    "        print (\"Vocabulary size: \",len(self.vocabulary))\n",
    "            \n",
    "        for index in range(len(self.classes)):\n",
    "            for word in self.vocabulary:\n",
    "                if word in wordCounts[index]:\n",
    "                    self.likelihoods[index][word] = np.log((wordCounts[index][word]+1)/(sum(wordCounts[index].values())+len(wordCounts[index])))\n",
    "                else:\n",
    "                    self.likelihoods[index][word] = np.log((1)/(sum(wordCounts[index].values())+len(wordCounts[index])))            \n",
    "        \n",
    "    def _read(self, document):\n",
    "        toReturn = []\n",
    "        with open(document) as f:\n",
    "            for line in f.readlines():\n",
    "                pair = line.split('\\n')\n",
    "                pair = pair[0].split('\\t')\n",
    "                review = re.sub(r\"[,?!-()*&^%|'.,]\",\"\",pair[0]) ## normalization\n",
    "                bag = self.preprocess(pair[0].lower())\n",
    "                label = int(pair[1])\n",
    "                toReturn.append((bag,label))\n",
    "        return toReturn\n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"\n",
    "        preprocesses the sentences. Tokenizes them, then lemmatizes the token using the wordnet lemmatizer\n",
    "        \"\"\"\n",
    "        import string\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        words = word_tokenize(sentence)\n",
    "        toReturn = []\n",
    "        for word in words:\n",
    "            if (word not in string.punctuation):\n",
    "                toReturn.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        return toReturn\n",
    "    \n",
    "    def train(self, documents, test=False, split_ratio=0.3):\n",
    "        \"\"\"\n",
    "        Takes txt inputs and trains the classifier\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for doc in documents:\n",
    "            print (\"reading: \",doc)\n",
    "            for review in self._read(doc):\n",
    "                corpus.append(review)\n",
    "                \n",
    "        if test:\n",
    "            np.random.shuffle(corpus)\n",
    "            split_point = int(len(corpus) * split_ratio)\n",
    "            test_data = corpus[:split_point]\n",
    "            train_data = corpus[split_point:]\n",
    "            self._train(train_data)\n",
    "            test_acc = self._test(test_data)\n",
    "            train_acc = self._test(train_data)\n",
    "            print (len(train_data),\" training items\")\n",
    "            print (len(test_data),\" testing items\")\n",
    "            print (\"Training done\")\n",
    "            print (\"Train accuracy: \",train_acc)\n",
    "            print (\"Test accuracy: \",test_acc)\n",
    "        else:\n",
    "            self._train(corpus)\n",
    "            print (\"Training done\")\n",
    "        self.trained = True\n",
    "        \n",
    "    def _predict(self, sentence):\n",
    "        \"\"\"\n",
    "        Takes tokenized input and outputs numerical class\n",
    "        \"\"\"\n",
    "        import operator\n",
    "        sumc = dict()\n",
    "        for c in range(self.nclasses):\n",
    "            sumc[c] = self.priors[c]\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary:\n",
    "                    sumc[c] += self.likelihoods[c][word]\n",
    "        return max(sumc.items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize sentence, predicts and output class\n",
    "        \"\"\"\n",
    "        sentence = self.preprocess(text)\n",
    "        return self._predict(sentence)\n",
    "    \n",
    "    def _test(self, data):\n",
    "        n_items = len(data)\n",
    "        n_correct = 0\n",
    "        for document in data:\n",
    "            review = document[0]\n",
    "            label = document[-1]\n",
    "            c = self._predict(review)\n",
    "            if (c == label): n_correct += 1\n",
    "        return n_correct / n_items  \n",
    "    \n",
    "    def export(self, name):\n",
    "        import json\n",
    "        \n",
    "        toExport = {\n",
    "            \"likelihoods\":self.likelihoods,\n",
    "            \"priors\":self.priors,\n",
    "            \"vocabulary\":self.vocabulary\n",
    "        }\n",
    "        \n",
    "        np.save(name, toExport)\n",
    "            \n",
    "    def load(self, name):\n",
    "        import json\n",
    "        \n",
    "        loaded = np.load(name)\n",
    "            \n",
    "        self.likelihoods = loaded.item().get(\"likelihoods\")\n",
    "        self.priors = loaded.item().get(\"priors\")\n",
    "        self.vocabulary = loaded.item().get(\"vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DacVtxlL8qLi"
   },
   "source": [
    "We train and test the classifier. We can see that even a naive classifier can achieve a relatively good accuracy on simple problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfWLsVhbXkDH",
    "outputId": "9c76ed28-b317-4b11-da64-57ffc9dc2681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading:  ./amazon_cells_labelled.txt\n",
      "reading:  ./imdb_labelled.txt\n",
      "reading:  ./yelp_labelled.txt\n",
      "Vocabulary size:  4371\n",
      "2400  training items\n",
      "600  testing items\n",
      "Training done\n",
      "Train accuracy:  0.93875\n",
      "Test accuracy:  0.83\n"
     ]
    }
   ],
   "source": [
    "classifier = naive_classifier(classes = [\"positive\", \"negative\"])\n",
    "classifier.train([\"./amazon_cells_labelled.txt\",\n",
    "                  \"./imdb_labelled.txt\",\n",
    "                  \"./yelp_labelled.txt\"],\n",
    "                 test=True,\n",
    "                 split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyZHuwcuVqSD"
   },
   "source": [
    "We can now test the classifier with any data at all and see the result. Here is an example that belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Tq4vDUb5Fn",
    "outputId": "e87bf73f-7f67-4767-c281-2294b1f20f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text1 = \"Mushoku Tensei is the greatest light novel series ever! It has great world building, foreshadowing and character development\"\n",
    "\n",
    "classifier.predict(test_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3X0Si9eVyVG"
   },
   "source": [
    "Here is one that belongs to the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdvLENFs1oVF",
    "outputId": "4127bc88-5e9c-4b03-f293-52d194ac9076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text2 = \"The new iPhone was not satisfactory. The innovation is not there anymore and prices are still through the roof\"\n",
    "\n",
    "classifier.predict(test_text2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2021-04-27-naive-bayes-classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
