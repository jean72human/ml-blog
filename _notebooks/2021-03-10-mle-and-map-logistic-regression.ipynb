{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abkauHd1F05X"
   },
   "source": [
    "# AMMI notes - MLE and MAP for logistic regression\n",
    "> maximum likelihood estimation and maximum a posteriori estimation for logistic regression\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Gbetondji Dovonon, Sebenele Thwala, Stephen Oni\n",
    "- categories: [notes,ammi,blog]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO47UoVIZro2"
   },
   "source": [
    "We did maximum likelihood estimation (MLE} and maximum a posteriori estimation (MAP) in the context of linear regression [here](https://jean72human.github.io/ml-blog/notes/ammi/blog/2021/03/05/mle-and-map.html). \n",
    "Let's do the same for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6u_awHDcSvA"
   },
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPgnHLoVc-lY"
   },
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCeJ46-_dF30"
   },
   "source": [
    "Let's say we have two classes represented by 0 and 1 and we are trying to predict the probability of a feature vector belonging to class 1, we would have:\n",
    "$$\n",
    "h_\\theta (x) = P(y=1|x;\\theta)\n",
    "\\\\\n",
    "1 - h_\\theta (x) = P(y=0|x;\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptELV3TGeirp"
   },
   "source": [
    "Since we have two possible outcomes (classes) we can use a Bernouilli distribution on the labels and get the likelihood and log likelihood:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|X;\\theta) & = h_\\theta (x)^y (1 - h_\\theta (x))^{1-y}\n",
    "\\\\\n",
    "\\log P(y|X;\\theta) & = y \\log h_\\theta (x) + (1-y) \\log (1 - h_\\theta (x))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XBEPQrkf-bP"
   },
   "source": [
    "Maximizing the log likelihood is the same as minimizing the negative log likelihood. From that we get our loss function to be:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} y_i \\log h_\\theta (x_i) + (1-y_i) \\log (1 - h_\\theta (x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWBQ1oMyj5y6"
   },
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S65whr3j-FL"
   },
   "source": [
    "In the case of multiclass classification we use the multinomial distribution instead of a Bernouilli one. Now, for m classes represented as $[1,2, ... ,m]$, we have $\\hat{y} = h_\\theta(x)$ to be a vector such that $\\hat{y}_k = P(y=k|x;\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw5D1d5KtDI3"
   },
   "source": [
    "For one data point $(x,y)$ we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|x;\\theta) & = \\prod_{k=1}^m \\frac{\\hat{y}_k^{y_k}}{y_k}\n",
    "\\\\\n",
    "\\log P(y|x;\\theta) & = \\sum_{k=1}^m \\log \\frac{\\hat{y}_k^{y_k}}{y_k}\n",
    "\\\\\n",
    "& = \\sum_{k=1}^m \\log \\hat{y}_k^{y_k} - \\sum_{k=1}^m y_k\n",
    "\\\\\n",
    "& = \\sum_{k=1}^m y_k \\log \\hat{y}_k - \\sum_{k=1}^m y_k\n",
    "\\\\\n",
    "& = \\sum_{k=1}^m y_k \\log \\hat{y}_k + c\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_zQoykVwbXd"
   },
   "source": [
    "Here $c$ is a constant not depending on $\\theta$ so it is not part of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tAEmsP_w4_n"
   },
   "source": [
    "We get our loss function to be:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^m y_k \\log h_\\theta(x)_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqWgB-idzn84"
   },
   "source": [
    "## Maximum a posteriori estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Dk4I0ezr5-"
   },
   "source": [
    "The MAP for logistic regression and linear regression are very similar. We consider $\\theta$ a random variable that follows a normal distribution $\\mathcal{N}(0,\\,b^{2})$. We want to maximize $P(\\theta|X,Y)$\n",
    "\n",
    "Using Baye's theorem we have \n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\theta|X,Y) & = \\frac{P(Y|X,\\theta)p(\\theta)}{P(Y|X)}\n",
    "\\\\\n",
    "\\ log P(\\theta|X,Y) & = \\log P(Y|X,\\theta) + \\log p(\\theta) - \\log P(Y|X)\n",
    "\\\\\n",
    "& = y \\log h_\\theta (x) + (1-y) \\log (1 - h_\\theta (x)) - n \\log \\sqrt{2 \\pi b^2} - \\frac{1}{2 b^2} \\|{\\theta }\\|_2^2 - \\log P(Y|X)\n",
    "\\\\\n",
    "& = (y \\log h_\\theta (x) + (1-y) \\log (1 - h_\\theta (x)) - \\frac{1}{2 b^2} \\|{\\theta }\\|_2^2 ) + c\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here $c$ is a constant that groups all the terms that do not depend on $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2RzV0Y-2kVT"
   },
   "source": [
    "From that we get our loss function to be:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -(\\frac{1}{N}\\sum_{i=1}^{N} y_i \\log h_\\theta (x_i) + (1-y_i) \\log (1 - h_\\theta (x_i)) + \\lambda \\| \\theta \\|_2^2)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mle-and-map-logistic-regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
